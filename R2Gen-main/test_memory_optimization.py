#!/usr/bin/env python3
"""
æµ‹è¯•æ˜¾å­˜ä¼˜åŒ–æ¨¡å—
"""

import sys
import os
import torch
import time

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from modules.memory_monitor import MemoryMonitor


class SimpleTestModel(torch.nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹ï¼Œæ¨¡æ‹ŸR2Gençš„æ˜¾å­˜ä½¿ç”¨"""
    def __init__(self):
        super().__init__()
        # æ¨¡æ‹Ÿè§†è§‰ç¼–ç å™¨
        self.visual_encoder = torch.nn.Sequential(
            torch.nn.Conv2d(3, 64, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.Conv2d(64, 128, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.AdaptiveAvgPool2d((14, 14))
        )
        
        # æ¨¡æ‹Ÿæ–‡æœ¬è§£ç å™¨
        self.text_decoder = torch.nn.Sequential(
            torch.nn.Linear(128 * 14 * 14, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 1000),  # è¯æ±‡è¡¨å¤§å°
        )
        
        # æ¨¡æ‹Ÿæ³¨æ„åŠ›æœºåˆ¶
        self.attention = torch.nn.MultiheadAttention(512, 8, batch_first=True)
    
    def forward(self, images, reports_ids=None, mode='train'):
        batch_size = images.size(0)
        
        # è§†è§‰ç‰¹å¾æå–
        visual_features = self.visual_encoder(images)
        visual_features = visual_features.view(batch_size, -1)
        
        # æ–‡æœ¬è§£ç 
        if mode == 'train' and reports_ids is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨teacher forcing
            seq_len = reports_ids.size(1)
            text_features = torch.randn(batch_size, seq_len, 512, device=images.device)
            
            # æ³¨æ„åŠ›è®¡ç®—
            attended_features, _ = self.attention(text_features, text_features, text_features)
            
            # è¾“å‡ºé¢„æµ‹
            output = self.text_decoder(visual_features.unsqueeze(1).expand(-1, seq_len, -1).reshape(-1, visual_features.size(1)))
            output = output.view(batch_size, seq_len, -1)
            
            return output
        else:
            # æ¨ç†æ¨¡å¼
            return self.text_decoder(visual_features)


def test_memory_monitor():
    """æµ‹è¯•æ˜¾å­˜ç›‘æ§å™¨"""
    print("ğŸ§ª æµ‹è¯•æ˜¾å­˜ç›‘æ§å™¨...")
    
    monitor = MemoryMonitor(target_utilization=0.85)
    monitor.print_status(detailed=True)
    
    return monitor


def test_batch_size_optimization():
    """æµ‹è¯•batch sizeä¼˜åŒ–"""
    print("\nğŸ§ª æµ‹è¯•Batch Sizeä¼˜åŒ–...")
    
    # åˆ›å»ºæ¨¡å‹å’Œç›‘æ§å™¨
    model = SimpleTestModel().cuda()
    monitor = MemoryMonitor(target_utilization=0.85)
    
    print(f"ğŸ“Š æ¨¡å‹åŠ è½½åæ˜¾å­˜çŠ¶æ€:")
    monitor.print_status()
    
    # æ‰‹åŠ¨æµ‹è¯•ä¸åŒbatch size
    test_sizes = [2, 4, 8, 12, 16, 20, 24]
    results = []
    
    print(f"\nğŸ”¬ å¼€å§‹batch sizeæµ‹è¯•...")
    
    for batch_size in test_sizes:
        print(f"\n  æµ‹è¯• batch_size = {batch_size}")
        
        try:
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            monitor.clear_cache_and_reset()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            images = torch.randn(batch_size, 3, 224, 224, device='cuda')
            reports_ids = torch.randint(0, 1000, (batch_size, 60), device='cuda')
            
            # è®°å½•å¼€å§‹çŠ¶æ€
            start_memory = monitor.get_current_usage()
            
            # å‰å‘ä¼ æ’­
            start_time = time.time()
            model.train()
            
            with torch.cuda.amp.autocast():  # ä½¿ç”¨FP16æµ‹è¯•
                output = model(images, reports_ids, mode='train')
                loss = output.mean()
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            end_memory = monitor.get_current_usage()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            step_time = end_time - start_time
            samples_per_sec = batch_size / step_time
            
            result = {
                'batch_size': batch_size,
                'success': True,
                'peak_memory_percent': end_memory['utilization_percent'],
                'peak_memory_gb': end_memory['reserved_gb'],
                'step_time': step_time,
                'samples_per_sec': samples_per_sec,
                'is_safe': end_memory['is_safe']
            }
            
            print(f"    âœ… æˆåŠŸ: {end_memory['utilization_percent']:.1f}% æ˜¾å­˜, {samples_per_sec:.1f} samples/sec")
            
            # æ¸…ç†
            del images, reports_ids, output, loss
            model.zero_grad()
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"    âŒ OOM: batch_size={batch_size}")
                result = {
                    'batch_size': batch_size,
                    'success': False,
                    'error': 'OOM',
                    'is_safe': False
                }
                
                # æ¸…ç†æ˜¾å­˜
                torch.cuda.empty_cache()
            else:
                print(f"    âŒ å…¶ä»–é”™è¯¯: {e}")
                result = {
                    'batch_size': batch_size,
                    'success': False,
                    'error': str(e),
                    'is_safe': False
                }
        
        results.append(result)
        
        # å¦‚æœå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•
        if not result['success']:
            break
    
    # åˆ†æç»“æœ
    successful_results = [r for r in results if r['success']]
    
    if successful_results:
        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        # ä¼˜å…ˆè€ƒè™‘å®‰å…¨æ€§ï¼Œç„¶åè€ƒè™‘æ˜¾å­˜åˆ©ç”¨ç‡
        safe_results = [r for r in successful_results if r['is_safe']]
        
        if safe_results:
            # åœ¨å®‰å…¨èŒƒå›´å†…æ‰¾åˆ°æ˜¾å­˜åˆ©ç”¨ç‡æœ€é«˜çš„
            optimal_result = max(safe_results, key=lambda x: x['peak_memory_percent'])
        else:
            # å¦‚æœæ²¡æœ‰å®‰å…¨çš„ï¼Œé€‰æ‹©æ˜¾å­˜ä½¿ç”¨æœ€å°‘çš„æˆåŠŸé…ç½®
            optimal_result = min(successful_results, key=lambda x: x['peak_memory_percent'])
        
        print(f"\nğŸ“Š ä¼˜åŒ–ç»“æœæ‘˜è¦:")
        print(f"   æµ‹è¯•èŒƒå›´: batch_size {test_sizes[0]} - {test_sizes[len(results)-1]}")
        print(f"   æˆåŠŸæµ‹è¯•: {len(successful_results)}/{len(results)}")
        print(f"   æœ€ä¼˜é…ç½®: batch_size = {optimal_result['batch_size']}")
        print(f"   æ˜¾å­˜ä½¿ç”¨: {optimal_result['peak_memory_percent']:.1f}% ({optimal_result['peak_memory_gb']:.2f}GB)")
        print(f"   è®­ç»ƒé€Ÿåº¦: {optimal_result['samples_per_sec']:.1f} samples/sec")
        print(f"   å®‰å…¨çŠ¶æ€: {'âœ… å®‰å…¨' if optimal_result['is_safe'] else 'âš ï¸ æ¥è¿‘ä¸Šé™'}")
        
        # è¯¦ç»†ç»“æœè¡¨æ ¼
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        print(f"{'Batch Size':<12} {'Memory %':<10} {'Memory GB':<12} {'Speed':<15} {'Status':<10}")
        print(f"{'-'*65}")
        
        for result in results:
            if result['success']:
                status = 'âœ… å®‰å…¨' if result['is_safe'] else 'âš ï¸ æ¥è¿‘'
                print(f"{result['batch_size']:<12} "
                      f"{result['peak_memory_percent']:<10.1f} "
                      f"{result['peak_memory_gb']:<12.2f} "
                      f"{result['samples_per_sec']:<15.1f} "
                      f"{status:<10}")
            else:
                print(f"{result['batch_size']:<12} "
                      f"{'N/A':<10} "
                      f"{'N/A':<12} "
                      f"{'N/A':<15} "
                      f"âŒ {result['error']:<10}")
        
        return optimal_result
    else:
        print(f"âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æ˜¾å­˜ä¼˜åŒ–æ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•1: æ˜¾å­˜ç›‘æ§å™¨
    monitor = test_memory_monitor()
    
    # æµ‹è¯•2: Batch sizeä¼˜åŒ–
    optimal_config = test_batch_size_optimization()
    
    # æœ€ç»ˆçŠ¶æ€
    print(f"\nğŸ“Š æœ€ç»ˆæ˜¾å­˜çŠ¶æ€:")
    monitor.print_status()
    
    if optimal_config:
        print(f"\nğŸ¯ æ¨èé…ç½®:")
        print(f"   batch_size = {optimal_config['batch_size']}")
        print(f"   é¢„æœŸæ˜¾å­˜ä½¿ç”¨: {optimal_config['peak_memory_percent']:.1f}%")
        print(f"   é¢„æœŸè®­ç»ƒé€Ÿåº¦: {optimal_config['samples_per_sec']:.1f} samples/sec")
        
        # è®¡ç®—ç›¸å¯¹äºbatch_size=4çš„æå‡
        baseline_speed = optimal_config['samples_per_sec'] * 4 / optimal_config['batch_size']  # ä¼°ç®—batch_size=4çš„é€Ÿåº¦
        speedup = optimal_config['samples_per_sec'] / baseline_speed
        
        print(f"   ç›¸å¯¹batch_size=4çš„æå‡: {speedup:.1f}å€")
    
    print(f"\nâœ… æ˜¾å­˜ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
