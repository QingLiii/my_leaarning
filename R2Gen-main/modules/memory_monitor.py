"""
æ˜¾å­˜ç›‘æ§æ¨¡å— - å®æ—¶ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
é’ˆå¯¹8GB RTX 4070 Laptop GPUä¼˜åŒ–
"""

import torch
import time
import numpy as np
from typing import Dict, List, Optional, Tuple
from collections import deque
import warnings

try:
    import GPUtil
    GPU_UTIL_AVAILABLE = True
except ImportError:
    GPU_UTIL_AVAILABLE = False
    warnings.warn("GPUtil not available, some GPU monitoring features will be disabled")


class MemoryMonitor:
    """
    GPUæ˜¾å­˜ç›‘æ§å™¨
    
    åŠŸèƒ½ï¼š
    - å®æ—¶ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    - æ£€æµ‹æ˜¾å­˜ä½¿ç”¨è¶‹åŠ¿
    - æä¾›å®‰å…¨æ€§æ£€æŸ¥
    - è®°å½•å†å²æ•°æ®ç”¨äºåˆ†æ
    """
    
    def __init__(self, 
                 target_utilization: float = 0.85,
                 device_id: int = 0,
                 history_size: int = 100):
        """
        åˆå§‹åŒ–æ˜¾å­˜ç›‘æ§å™¨
        
        Args:
            target_utilization: ç›®æ ‡æ˜¾å­˜åˆ©ç”¨ç‡ (0.85 = 85%)
            device_id: GPUè®¾å¤‡ID
            history_size: å†å²è®°å½•ä¿å­˜æ•°é‡
        """
        self.device_id = device_id
        self.target_utilization = target_utilization
        self.history_size = history_size
        
        # è·å–GPUåŸºç¡€ä¿¡æ¯
        if torch.cuda.is_available():
            self.device_props = torch.cuda.get_device_properties(device_id)
            self.total_memory = self.device_props.total_memory
            self.total_memory_gb = self.total_memory / (1024**3)
            self.safe_memory = self.total_memory * target_utilization
            self.safe_memory_gb = self.safe_memory / (1024**3)
        else:
            raise RuntimeError("CUDA not available")
        
        # å†å²è®°å½•
        self.memory_history = deque(maxlen=history_size)
        self.timestamp_history = deque(maxlen=history_size)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.peak_memory_allocated = 0
        self.peak_memory_reserved = 0
        self.oom_count = 0
        self.last_check_time = time.time()
        
        print(f"âœ… æ˜¾å­˜ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   GPU: {self.device_props.name}")
        print(f"   æ€»æ˜¾å­˜: {self.total_memory_gb:.2f} GB")
        print(f"   ç›®æ ‡ä½¿ç”¨ç‡: {target_utilization*100:.1f}%")
        print(f"   å®‰å…¨é˜ˆå€¼: {self.safe_memory_gb:.2f} GB")
    
    def get_current_usage(self) -> Dict[str, float]:
        """
        è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        
        Returns:
            åŒ…å«è¯¦ç»†æ˜¾å­˜ä¿¡æ¯çš„å­—å…¸
        """
        current_time = time.time()
        
        # PyTorchæ˜¾å­˜ä¿¡æ¯
        allocated = torch.cuda.memory_allocated(self.device_id)
        reserved = torch.cuda.memory_reserved(self.device_id)
        free_pytorch = self.total_memory - reserved
        
        # è®¡ç®—ä½¿ç”¨ç‡
        allocated_percent = (allocated / self.total_memory) * 100
        reserved_percent = (reserved / self.total_memory) * 100
        free_percent = (free_pytorch / self.total_memory) * 100
        
        usage_info = {
            # ç»å¯¹å€¼ (å­—èŠ‚)
            'allocated_bytes': allocated,
            'reserved_bytes': reserved,
            'free_bytes': free_pytorch,
            'total_bytes': self.total_memory,
            
            # GBå•ä½
            'allocated_gb': allocated / (1024**3),
            'reserved_gb': reserved / (1024**3),
            'free_gb': free_pytorch / (1024**3),
            'total_gb': self.total_memory_gb,
            
            # ç™¾åˆ†æ¯”
            'allocated_percent': allocated_percent,
            'reserved_percent': reserved_percent,
            'free_percent': free_percent,
            'utilization_percent': reserved_percent,  # ä¸»è¦ä½¿ç”¨ç‡æŒ‡æ ‡
            
            # æ—¶é—´æˆ³
            'timestamp': current_time,
            
            # å®‰å…¨æ€§æŒ‡æ ‡
            'is_safe': reserved < self.safe_memory,
            'safety_margin_gb': (self.safe_memory - reserved) / (1024**3),
            'safety_margin_percent': ((self.safe_memory - reserved) / self.total_memory) * 100
        }
        
        # æ›´æ–°å³°å€¼è®°å½•
        if allocated > self.peak_memory_allocated:
            self.peak_memory_allocated = allocated
        if reserved > self.peak_memory_reserved:
            self.peak_memory_reserved = reserved
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.memory_history.append(usage_info.copy())
        self.timestamp_history.append(current_time)
        
        # æ›´æ–°æ£€æŸ¥æ—¶é—´
        self.last_check_time = current_time
        
        return usage_info
    
    def get_gpu_util_info(self) -> Optional[Dict[str, float]]:
        """
        ä½¿ç”¨GPUtilè·å–é¢å¤–çš„GPUä¿¡æ¯
        
        Returns:
            GPUåˆ©ç”¨ç‡ã€æ¸©åº¦ç­‰ä¿¡æ¯ï¼Œå¦‚æœGPUtilä¸å¯ç”¨åˆ™è¿”å›None
        """
        if not GPU_UTIL_AVAILABLE:
            return None
        
        try:
            gpus = GPUtil.getGPUs()
            if gpus and len(gpus) > self.device_id:
                gpu = gpus[self.device_id]
                return {
                    'gpu_utilization_percent': gpu.load * 100,
                    'gpu_memory_used_mb': gpu.memoryUsed,
                    'gpu_memory_total_mb': gpu.memoryTotal,
                    'gpu_memory_free_mb': gpu.memoryFree,
                    'gpu_memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,
                    'gpu_temperature_c': gpu.temperature,
                    'gpu_name': gpu.name,
                    'gpu_uuid': gpu.uuid
                }
        except Exception as e:
            warnings.warn(f"Failed to get GPU util info: {e}")
        
        return None
    
    def get_comprehensive_status(self) -> Dict[str, any]:
        """
        è·å–ç»¼åˆçš„æ˜¾å­˜å’ŒGPUçŠ¶æ€ä¿¡æ¯
        
        Returns:
            åŒ…å«PyTorchå’ŒGPUtilä¿¡æ¯çš„ç»¼åˆçŠ¶æ€
        """
        status = {
            'memory': self.get_current_usage(),
            'gpu_util': self.get_gpu_util_info(),
            'statistics': self.get_statistics(),
            'trends': self.get_trends()
        }
        
        return status
    
    def get_statistics(self) -> Dict[str, float]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            å³°å€¼ä½¿ç”¨ã€å¹³å‡ä½¿ç”¨ç­‰ç»Ÿè®¡æ•°æ®
        """
        stats = {
            'peak_allocated_gb': self.peak_memory_allocated / (1024**3),
            'peak_reserved_gb': self.peak_memory_reserved / (1024**3),
            'peak_allocated_percent': (self.peak_memory_allocated / self.total_memory) * 100,
            'peak_reserved_percent': (self.peak_memory_reserved / self.total_memory) * 100,
            'oom_count': self.oom_count,
            'monitoring_duration_minutes': (time.time() - self.timestamp_history[0]) / 60 if self.timestamp_history else 0
        }
        
        # è®¡ç®—å†å²å¹³å‡å€¼
        if len(self.memory_history) > 0:
            recent_usage = [entry['utilization_percent'] for entry in self.memory_history]
            stats.update({
                'avg_utilization_percent': np.mean(recent_usage),
                'max_utilization_percent': np.max(recent_usage),
                'min_utilization_percent': np.min(recent_usage),
                'std_utilization_percent': np.std(recent_usage)
            })
        
        return stats
    
    def get_trends(self) -> Dict[str, any]:
        """
        åˆ†ææ˜¾å­˜ä½¿ç”¨è¶‹åŠ¿
        
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        if len(self.memory_history) < 10:
            return {'trend': 'insufficient_data', 'slope': 0, 'prediction': None}
        
        # è·å–æœ€è¿‘çš„ä½¿ç”¨ç‡æ•°æ®
        recent_usage = [entry['utilization_percent'] for entry in list(self.memory_history)[-20:]]
        recent_times = list(range(len(recent_usage)))
        
        # è®¡ç®—è¶‹åŠ¿æ–œç‡
        slope = np.polyfit(recent_times, recent_usage, 1)[0]
        
        # åˆ¤æ–­è¶‹åŠ¿
        if abs(slope) < 0.1:
            trend = 'stable'
        elif slope > 0.1:
            trend = 'increasing'
        else:
            trend = 'decreasing'
        
        # é¢„æµ‹æœªæ¥ä½¿ç”¨ç‡ï¼ˆç®€å•çº¿æ€§é¢„æµ‹ï¼‰
        if len(recent_usage) > 0:
            current_usage = recent_usage[-1]
            predicted_usage_10_steps = current_usage + slope * 10
            predicted_usage_10_steps = max(0, min(100, predicted_usage_10_steps))
        else:
            predicted_usage_10_steps = None
        
        return {
            'trend': trend,
            'slope': slope,
            'current_usage': recent_usage[-1] if recent_usage else 0,
            'predicted_usage_10_steps': predicted_usage_10_steps,
            'is_memory_leak_suspected': slope > 1.0,  # æ¯æ­¥å¢é•¿è¶…è¿‡1%å¯èƒ½æ˜¯å†…å­˜æ³„æ¼
            'samples_count': len(recent_usage)
        }
    
    def is_safe_to_increase(self, estimated_increase_gb: float) -> Tuple[bool, Dict[str, any]]:
        """
        åˆ¤æ–­æ˜¯å¦å¯ä»¥å®‰å…¨å¢åŠ æ˜¾å­˜ä½¿ç”¨
        
        Args:
            estimated_increase_gb: é¢„è®¡å¢åŠ çš„æ˜¾å­˜ä½¿ç”¨é‡(GB)
            
        Returns:
            (æ˜¯å¦å®‰å…¨, è¯¦ç»†ä¿¡æ¯)
        """
        current = self.get_current_usage()
        projected_usage_gb = current['reserved_gb'] + estimated_increase_gb
        projected_percent = (projected_usage_gb / self.total_memory_gb) * 100
        
        is_safe = projected_usage_gb <= self.safe_memory_gb
        
        safety_info = {
            'current_usage_gb': current['reserved_gb'],
            'estimated_increase_gb': estimated_increase_gb,
            'projected_usage_gb': projected_usage_gb,
            'projected_percent': projected_percent,
            'target_percent': self.target_utilization * 100,
            'is_safe': is_safe,
            'margin_gb': self.safe_memory_gb - projected_usage_gb,
            'margin_percent': ((self.safe_memory_gb - projected_usage_gb) / self.total_memory_gb) * 100
        }
        
        return is_safe, safety_info
    
    def record_oom_event(self, context: str = ""):
        """
        è®°å½•OOMäº‹ä»¶
        
        Args:
            context: OOMå‘ç”Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        self.oom_count += 1
        oom_info = {
            'timestamp': time.time(),
            'context': context,
            'memory_state': self.get_current_usage(),
            'oom_count': self.oom_count
        }
        
        print(f"ğŸš¨ OOMäº‹ä»¶è®°å½• #{self.oom_count}")
        print(f"   æ—¶é—´: {time.strftime('%H:%M:%S')}")
        print(f"   ä¸Šä¸‹æ–‡: {context}")
        print(f"   æ˜¾å­˜ä½¿ç”¨: {oom_info['memory_state']['utilization_percent']:.1f}%")
        
        return oom_info
    
    def clear_cache_and_reset(self):
        """
        æ¸…ç†æ˜¾å­˜ç¼“å­˜å¹¶é‡ç½®ç»Ÿè®¡
        """
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats(self.device_id)
        
        # é‡ç½®å³°å€¼ç»Ÿè®¡
        self.peak_memory_allocated = torch.cuda.memory_allocated(self.device_id)
        self.peak_memory_reserved = torch.cuda.memory_reserved(self.device_id)
        
        print("ğŸ§¹ æ˜¾å­˜ç¼“å­˜å·²æ¸…ç†ï¼Œç»Ÿè®¡å·²é‡ç½®")
    
    def print_status(self, detailed: bool = False):
        """
        æ‰“å°å½“å‰çŠ¶æ€
        
        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        status = self.get_comprehensive_status()
        memory = status['memory']
        gpu_util = status['gpu_util']
        stats = status['statistics']
        trends = status['trends']
        
        print(f"\nğŸ“Š æ˜¾å­˜ç›‘æ§çŠ¶æ€ ({time.strftime('%H:%M:%S')})")
        print(f"   æ˜¾å­˜ä½¿ç”¨: {memory['utilization_percent']:.1f}% ({memory['reserved_gb']:.2f}GB / {memory['total_gb']:.2f}GB)")
        print(f"   å®‰å…¨çŠ¶æ€: {'âœ… å®‰å…¨' if memory['is_safe'] else 'âš ï¸ æ¥è¿‘ä¸Šé™'}")
        print(f"   å®‰å…¨ä½™é‡: {memory['safety_margin_gb']:.2f}GB ({memory['safety_margin_percent']:.1f}%)")
        
        if gpu_util:
            print(f"   GPUåˆ©ç”¨ç‡: {gpu_util['gpu_utilization_percent']:.1f}%")
            print(f"   GPUæ¸©åº¦: {gpu_util['gpu_temperature_c']}Â°C")
        
        if detailed:
            print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   å³°å€¼ä½¿ç”¨: {stats['peak_reserved_percent']:.1f}%")
            print(f"   å¹³å‡ä½¿ç”¨: {stats.get('avg_utilization_percent', 0):.1f}%")
            print(f"   ä½¿ç”¨è¶‹åŠ¿: {trends['trend']}")
            if trends.get('is_memory_leak_suspected', False):
                print(f"   âš ï¸ ç–‘ä¼¼å†…å­˜æ³„æ¼ (æ–œç‡: {trends['slope']:.2f})")


if __name__ == "__main__":
    # æµ‹è¯•æ˜¾å­˜ç›‘æ§å™¨
    print("ğŸ§ª æµ‹è¯•æ˜¾å­˜ç›‘æ§å™¨...")
    
    monitor = MemoryMonitor()
    
    # åŸºç¡€çŠ¶æ€æ£€æŸ¥
    monitor.print_status(detailed=True)
    
    # æ¨¡æ‹Ÿä¸€äº›æ˜¾å­˜ä½¿ç”¨
    print("\nğŸ”¬ æ¨¡æ‹Ÿæ˜¾å­˜ä½¿ç”¨æµ‹è¯•...")
    test_tensors = []
    
    for i in range(5):
        # åˆ›å»ºä¸€äº›å¼ é‡æ¥ä½¿ç”¨æ˜¾å­˜
        tensor = torch.randn(1000, 1000, device='cuda')
        test_tensors.append(tensor)
        
        # æ£€æŸ¥çŠ¶æ€
        status = monitor.get_current_usage()
        print(f"   æ­¥éª¤ {i+1}: {status['utilization_percent']:.1f}% æ˜¾å­˜ä½¿ç”¨")
        
        # æµ‹è¯•å®‰å…¨æ€§æ£€æŸ¥
        is_safe, safety_info = monitor.is_safe_to_increase(0.5)  # æµ‹è¯•å¢åŠ 500MB
        print(f"   å¢åŠ 500MBå®‰å…¨æ€§: {'âœ…' if is_safe else 'âŒ'}")
    
    # æ¸…ç†æµ‹è¯•å¼ é‡
    del test_tensors
    monitor.clear_cache_and_reset()
    
    # æœ€ç»ˆçŠ¶æ€
    print("\nğŸ“Š æ¸…ç†åçŠ¶æ€:")
    monitor.print_status()
    
    print("âœ… æ˜¾å­˜ç›‘æ§å™¨æµ‹è¯•å®Œæˆï¼")
