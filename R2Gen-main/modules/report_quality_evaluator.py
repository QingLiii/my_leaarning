"""
æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨
å¯¹ä¸åŒç²¾åº¦æ¨¡å‹ç”Ÿæˆçš„åŒ»å­¦æŠ¥å‘Šè¿›è¡Œå®šæ€§å’Œå®šé‡è¯„ä¼°
"""

import torch
import json
import os
import time
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns


class ReportQualityEvaluator:
    """
    æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨
    
    åŠŸèƒ½ï¼š
    - å¯¹æ¯”ä¸åŒç²¾åº¦æ¨¡å‹çš„ç”Ÿæˆè´¨é‡
    - ç”Ÿæˆæ ·æœ¬æŠ¥å‘Šå±•ç¤º
    - æä¾›å¤šç»´åº¦è´¨é‡åˆ†æ
    - ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š
    """
    
    def __init__(self, 
                 models_dict: Dict[str, torch.nn.Module],
                 tokenizer,
                 device: str = 'cuda:0'):
        """
        åˆå§‹åŒ–æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨
        
        Args:
            models_dict: ä¸åŒç²¾åº¦çš„æ¨¡å‹å­—å…¸ {'fp32': model, 'fp16': model, ...}
            tokenizer: åˆ†è¯å™¨
            device: è®¾å¤‡
        """
        self.models_dict = models_dict
        self.tokenizer = tokenizer
        self.device = device
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.evaluation_results = {}
        self.sample_reports = {}
        
        # è´¨é‡è¯„ä¼°ç»´åº¦
        self.quality_dimensions = {
            'medical_accuracy': 'åŒ»å­¦å‡†ç¡®æ€§',
            'completeness': 'å®Œæ•´æ€§',
            'coherence': 'è¿è´¯æ€§',
            'specificity': 'å…·ä½“æ€§',
            'clinical_relevance': 'ä¸´åºŠç›¸å…³æ€§'
        }
        
        print(f"âœ… æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹æ•°é‡: {len(models_dict)}")
        print(f"   è¯„ä¼°ç»´åº¦: {len(self.quality_dimensions)}")
    
    def generate_sample_reports(self, 
                               test_images: List[torch.Tensor],
                               test_image_ids: List[str],
                               ground_truth_reports: List[str],
                               num_samples: int = 5) -> Dict[str, List[Dict]]:
        """
        ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ ·æœ¬æŠ¥å‘Š
        
        Args:
            test_images: æµ‹è¯•å›¾åƒåˆ—è¡¨
            test_image_ids: å›¾åƒIDåˆ—è¡¨
            ground_truth_reports: çœŸå®æŠ¥å‘Šåˆ—è¡¨
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            
        Returns:
            æ¯ä¸ªæ¨¡å‹çš„æ ·æœ¬æŠ¥å‘Šå­—å…¸
        """
        print(f"\nğŸ”¬ å¼€å§‹ç”Ÿæˆæ ·æœ¬æŠ¥å‘Š (æ¯ä¸ªæ¨¡å‹{num_samples}ä¸ªæ ·æœ¬)...")
        
        # é€‰æ‹©æ ·æœ¬
        sample_indices = np.random.choice(len(test_images), num_samples, replace=False)
        
        results = {}
        
        for model_name, model in self.models_dict.items():
            print(f"\n  ğŸ“ ç”Ÿæˆ {model_name} æ¨¡å‹æŠ¥å‘Š...")
            model.eval()
            
            model_samples = []
            
            with torch.no_grad():
                for i, idx in enumerate(sample_indices):
                    print(f"    æ ·æœ¬ {i+1}/{num_samples}: å›¾åƒ {test_image_ids[idx]}")
                    
                    # å‡†å¤‡è¾“å…¥
                    image = test_images[idx].unsqueeze(0).to(self.device)
                    
                    # ç”ŸæˆæŠ¥å‘Š
                    start_time = time.time()
                    
                    if model_name == 'fp16':
                        with torch.cuda.amp.autocast():
                            generated_ids = model(image, mode='sample')
                    else:
                        generated_ids = model(image, mode='sample')
                    
                    generation_time = time.time() - start_time
                    
                    # è§£ç ç”Ÿæˆçš„æŠ¥å‘Š
                    if isinstance(generated_ids, list):
                        generated_report = generated_ids[0]  # å–ç¬¬ä¸€ä¸ªç”Ÿæˆç»“æœ
                    else:
                        generated_report = self.tokenizer.decode_batch(generated_ids.cpu().numpy())[0]
                    
                    # æ¸…ç†æŠ¥å‘Šæ–‡æœ¬
                    generated_report = self._clean_report_text(generated_report)
                    ground_truth = self._clean_report_text(ground_truth_reports[idx])
                    
                    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
                    metrics = self._calculate_basic_metrics(generated_report, ground_truth)
                    
                    sample_data = {
                        'image_id': test_image_ids[idx],
                        'sample_index': i + 1,
                        'generated_report': generated_report,
                        'ground_truth_report': ground_truth,
                        'generation_time_ms': generation_time * 1000,
                        'metrics': metrics,
                        'model_precision': model_name
                    }
                    
                    model_samples.append(sample_data)
                    
                    print(f"      ç”Ÿæˆæ—¶é—´: {generation_time*1000:.1f}ms")
                    print(f"      BLEU-4: {metrics['bleu4']:.3f}")
            
            results[model_name] = model_samples
            print(f"  âœ… {model_name} æ¨¡å‹æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        self.sample_reports = results
        return results
    
    def _clean_report_text(self, text: str) -> str:
        """æ¸…ç†æŠ¥å‘Šæ–‡æœ¬"""
        if isinstance(text, list):
            text = ' '.join(text)
        
        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        text = text.replace('<unk>', '').replace('<pad>', '').replace('<eos>', '')
        text = text.replace('<bos>', '').replace('<start>', '').replace('<end>', '')
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _calculate_basic_metrics(self, generated: str, reference: str) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€æŒ‡æ ‡"""
        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
        from nltk.tokenize import word_tokenize
        
        try:
            # åˆ†è¯
            gen_tokens = word_tokenize(generated.lower())
            ref_tokens = word_tokenize(reference.lower())
            
            # BLEUåˆ†æ•°
            smoothie = SmoothingFunction().method4
            bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)
            bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)
            
            # é•¿åº¦æ¯”è¾ƒ
            length_ratio = len(gen_tokens) / len(ref_tokens) if len(ref_tokens) > 0 else 0
            
            # è¯æ±‡é‡å 
            gen_set = set(gen_tokens)
            ref_set = set(ref_tokens)
            vocab_overlap = len(gen_set & ref_set) / len(ref_set) if len(ref_set) > 0 else 0
            
            return {
                'bleu1': bleu1,
                'bleu4': bleu4,
                'length_ratio': length_ratio,
                'vocab_overlap': vocab_overlap,
                'generated_length': len(gen_tokens),
                'reference_length': len(ref_tokens)
            }
        except Exception as e:
            print(f"âš ï¸ æŒ‡æ ‡è®¡ç®—é”™è¯¯: {e}")
            return {
                'bleu1': 0.0, 'bleu4': 0.0, 'length_ratio': 0.0, 
                'vocab_overlap': 0.0, 'generated_length': 0, 'reference_length': 0
            }
    
    def analyze_report_quality(self) -> Dict[str, Any]:
        """
        åˆ†ææŠ¥å‘Šè´¨é‡
        
        Returns:
            è´¨é‡åˆ†æç»“æœ
        """
        if not self.sample_reports:
            raise ValueError("è¯·å…ˆç”Ÿæˆæ ·æœ¬æŠ¥å‘Š")
        
        print(f"\nğŸ“Š å¼€å§‹æŠ¥å‘Šè´¨é‡åˆ†æ...")
        
        analysis_results = {}
        
        for model_name, samples in self.sample_reports.items():
            print(f"\n  ğŸ” åˆ†æ {model_name} æ¨¡å‹...")
            
            # æ”¶é›†æŒ‡æ ‡
            metrics_list = [sample['metrics'] for sample in samples]
            generation_times = [sample['generation_time_ms'] for sample in samples]
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            model_analysis = {
                'model_name': model_name,
                'num_samples': len(samples),
                'avg_bleu1': np.mean([m['bleu1'] for m in metrics_list]),
                'avg_bleu4': np.mean([m['bleu4'] for m in metrics_list]),
                'avg_length_ratio': np.mean([m['length_ratio'] for m in metrics_list]),
                'avg_vocab_overlap': np.mean([m['vocab_overlap'] for m in metrics_list]),
                'avg_generation_time_ms': np.mean(generation_times),
                'std_generation_time_ms': np.std(generation_times),
                'avg_generated_length': np.mean([m['generated_length'] for m in metrics_list]),
                'avg_reference_length': np.mean([m['reference_length'] for m in metrics_list])
            }
            
            # è´¨é‡è¯„åˆ† (ç»¼åˆæŒ‡æ ‡)
            quality_score = (
                model_analysis['avg_bleu4'] * 0.4 +
                model_analysis['avg_vocab_overlap'] * 0.3 +
                min(1.0, model_analysis['avg_length_ratio']) * 0.3
            )
            model_analysis['quality_score'] = quality_score
            
            analysis_results[model_name] = model_analysis
            
            print(f"    BLEU-4: {model_analysis['avg_bleu4']:.3f}")
            print(f"    è¯æ±‡é‡å : {model_analysis['avg_vocab_overlap']:.3f}")
            print(f"    é•¿åº¦æ¯”ä¾‹: {model_analysis['avg_length_ratio']:.3f}")
            print(f"    è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            print(f"    ç”Ÿæˆé€Ÿåº¦: {model_analysis['avg_generation_time_ms']:.1f}ms")
        
        self.evaluation_results = analysis_results
        return analysis_results
    
    def generate_comparison_report(self, save_path: str = "report_quality_comparison.html") -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            ç”Ÿæˆçš„HTMLæŠ¥å‘Šè·¯å¾„
        """
        if not self.sample_reports or not self.evaluation_results:
            raise ValueError("è¯·å…ˆç”Ÿæˆæ ·æœ¬æŠ¥å‘Šå’Œè´¨é‡åˆ†æ")
        
        print(f"\nğŸ“„ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        html_content = self._create_html_report()
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
        return save_path
    
    def _create_html_report(self) -> str:
        """åˆ›å»ºHTMLæŠ¥å‘Š"""
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        model_names = ', '.join(self.sample_reports.keys())
        num_samples = len(list(self.sample_reports.values())[0])

        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>R2Genæ¨¡å‹æŠ¥å‘Šè´¨é‡å¯¹æ¯”</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: #f4f4f4; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
        .model-section {{ border: 1px solid #ddd; margin: 20px 0; padding: 15px; border-radius: 5px; }}
        .fp32 {{ border-left: 5px solid #3498db; }}
        .fp16 {{ border-left: 5px solid #e74c3c; }}
        .fp8 {{ border-left: 5px solid #2ecc71; }}
        .sample {{ background: #f9f9f9; margin: 10px 0; padding: 15px; border-radius: 3px; }}
        .metrics {{ display: flex; gap: 20px; margin: 10px 0; }}
        .metric {{ background: white; padding: 10px; border-radius: 3px; text-align: center; }}
        .generated {{ background: #e8f5e8; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .reference {{ background: #f0f8ff; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .comparison-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .comparison-table th, .comparison-table td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}
        .comparison-table th {{ background: #f4f4f4; }}
        .best {{ background: #d4edda; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¥ R2Genæ¨¡å‹æŠ¥å‘Šè´¨é‡å¯¹æ¯”åˆ†æ</h1>
        <p>ç”Ÿæˆæ—¶é—´: {timestamp}</p>
        <p>å¯¹æ¯”æ¨¡å‹: {model_names}</p>
        <p>æ ·æœ¬æ•°é‡: {num_samples} ä¸ª/æ¨¡å‹</p>
    </div>
"""
        
        # æ·»åŠ æ€»ä½“å¯¹æ¯”è¡¨æ ¼
        html += self._create_summary_table()
        
        # æ·»åŠ è¯¦ç»†æ ·æœ¬å¯¹æ¯”
        html += self._create_detailed_samples()
        
        html += """
</body>
</html>
"""
        return html
    
    def _create_summary_table(self) -> str:
        """åˆ›å»ºæ€»ä½“å¯¹æ¯”è¡¨æ ¼"""
        html = """
    <h2>ğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”</h2>
    <table class="comparison-table">
        <tr>
            <th>æ¨¡å‹</th>
            <th>BLEU-1</th>
            <th>BLEU-4</th>
            <th>è¯æ±‡é‡å </th>
            <th>é•¿åº¦æ¯”ä¾‹</th>
            <th>è´¨é‡è¯„åˆ†</th>
            <th>ç”Ÿæˆé€Ÿåº¦(ms)</th>
        </tr>
"""
        
        # æ‰¾åˆ°æœ€ä½³å€¼ç”¨äºé«˜äº®
        best_bleu4 = max(result['avg_bleu4'] for result in self.evaluation_results.values())
        best_quality = max(result['quality_score'] for result in self.evaluation_results.values())
        best_speed = min(result['avg_generation_time_ms'] for result in self.evaluation_results.values())
        
        for model_name, result in self.evaluation_results.items():
            bleu4_class = 'best' if result['avg_bleu4'] == best_bleu4 else ''
            quality_class = 'best' if result['quality_score'] == best_quality else ''
            speed_class = 'best' if result['avg_generation_time_ms'] == best_speed else ''
            
            html += f"""
        <tr>
            <td><strong>{model_name.upper()}</strong></td>
            <td>{result['avg_bleu1']:.3f}</td>
            <td class="{bleu4_class}">{result['avg_bleu4']:.3f}</td>
            <td>{result['avg_vocab_overlap']:.3f}</td>
            <td>{result['avg_length_ratio']:.3f}</td>
            <td class="{quality_class}">{result['quality_score']:.3f}</td>
            <td class="{speed_class}">{result['avg_generation_time_ms']:.1f}</td>
        </tr>
"""
        
        html += """
    </table>
"""
        return html
    
    def _create_detailed_samples(self) -> str:
        """åˆ›å»ºè¯¦ç»†æ ·æœ¬å¯¹æ¯”"""
        html = "<h2>ğŸ“ è¯¦ç»†æ ·æœ¬å¯¹æ¯”</h2>\n"
        
        # æŒ‰æ ·æœ¬ç´¢å¼•ç»„ç»‡æ•°æ®
        num_samples = len(list(self.sample_reports.values())[0])
        
        for sample_idx in range(num_samples):
            html += f"<h3>æ ·æœ¬ {sample_idx + 1}</h3>\n"
            
            # æ˜¾ç¤ºçœŸå®æŠ¥å‘Š
            reference_report = list(self.sample_reports.values())[0][sample_idx]['ground_truth_report']
            html += f'<div class="reference"><strong>çœŸå®æŠ¥å‘Š:</strong><br>{reference_report}</div>\n'
            
            # æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„ç”Ÿæˆç»“æœ
            for model_name in self.sample_reports.keys():
                sample = self.sample_reports[model_name][sample_idx]
                
                html += f"""
<div class="model-section {model_name}">
    <h4>{model_name.upper()} æ¨¡å‹ç”Ÿæˆ</h4>
    <div class="generated"><strong>ç”ŸæˆæŠ¥å‘Š:</strong><br>{sample['generated_report']}</div>
    <div class="metrics">
        <div class="metric">
            <strong>BLEU-4</strong><br>
            {sample['metrics']['bleu4']:.3f}
        </div>
        <div class="metric">
            <strong>è¯æ±‡é‡å </strong><br>
            {sample['metrics']['vocab_overlap']:.3f}
        </div>
        <div class="metric">
            <strong>é•¿åº¦æ¯”ä¾‹</strong><br>
            {sample['metrics']['length_ratio']:.3f}
        </div>
        <div class="metric">
            <strong>ç”Ÿæˆæ—¶é—´</strong><br>
            {sample['generation_time_ms']:.1f}ms
        </div>
    </div>
</div>
"""
        
        return html
    
    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        if not self.evaluation_results:
            print("âŒ è¯·å…ˆè¿è¡Œè´¨é‡åˆ†æ")
            return
        
        print(f"\nğŸ“Š æŠ¥å‘Šè´¨é‡è¯„ä¼°æ‘˜è¦")
        print(f"=" * 60)
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        sorted_results = sorted(
            self.evaluation_results.items(),
            key=lambda x: x[1]['quality_score'],
            reverse=True
        )
        
        print(f"{'æ¨¡å‹':<8} {'è´¨é‡è¯„åˆ†':<10} {'BLEU-4':<10} {'ç”Ÿæˆé€Ÿåº¦':<12} {'ç»¼åˆæ’å'}")
        print(f"{'-'*60}")
        
        for rank, (model_name, result) in enumerate(sorted_results, 1):
            print(f"{model_name.upper():<8} "
                  f"{result['quality_score']:<10.3f} "
                  f"{result['avg_bleu4']:<10.3f} "
                  f"{result['avg_generation_time_ms']:<12.1f} "
                  f"#{rank}")
        
        # æœ€ä½³æ¨¡å‹æ¨è
        best_model = sorted_results[0][0]
        print(f"\nğŸ† æ¨èæ¨¡å‹: {best_model.upper()}")
        print(f"   ç†ç”±: åœ¨è´¨é‡è¯„åˆ†ä¸­è¡¨ç°æœ€ä½³")
        
        # é€Ÿåº¦æœ€å¿«æ¨¡å‹
        fastest_model = min(
            self.evaluation_results.items(),
            key=lambda x: x[1]['avg_generation_time_ms']
        )[0]
        print(f"âš¡ é€Ÿåº¦æœ€å¿«: {fastest_model.upper()}")


if __name__ == "__main__":
    print("ğŸ§ª æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨æµ‹è¯•...")
    print("æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¡†æ¶æµ‹è¯•ï¼Œéœ€è¦å®é™…çš„æ¨¡å‹å’Œæ•°æ®æ¥è¿è¡Œå®Œæ•´è¯„ä¼°")
    
    # è¿™é‡Œåªæ˜¯å±•ç¤ºä½¿ç”¨æ–¹æ³•
    print("""
ä½¿ç”¨ç¤ºä¾‹:
    # 1. å‡†å¤‡ä¸åŒç²¾åº¦çš„æ¨¡å‹
    models = {
        'fp32': model_fp32,
        'fp16': model_fp16,
        'fp8': model_fp8
    }
    
    # 2. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ReportQualityEvaluator(models, tokenizer)
    
    # 3. ç”Ÿæˆæ ·æœ¬æŠ¥å‘Š
    evaluator.generate_sample_reports(test_images, image_ids, ground_truth)
    
    # 4. åˆ†æè´¨é‡
    evaluator.analyze_report_quality()
    
    # 5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    evaluator.generate_comparison_report()
    
    # 6. æ‰“å°æ‘˜è¦
    evaluator.print_summary()
    """)
