"""
åŠ¨æ€Batch Sizeä¼˜åŒ–å™¨
è‡ªåŠ¨æ‰¾åˆ°æœ€é€‚åˆ8GBæ˜¾å­˜çš„æœ€ä¼˜batch sizeé…ç½®
"""

import torch
import time
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import warnings

try:
    from .memory_monitor import MemoryMonitor
except ImportError:
    from memory_monitor import MemoryMonitor


class BatchSizeOptimizer:
    """
    åŠ¨æ€Batch Sizeä¼˜åŒ–å™¨
    
    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨æµ‹è¯•ä¸åŒbatch sizeçš„æ˜¾å­˜ä½¿ç”¨
    - æ‰¾åˆ°æœ€ä¼˜batch sizeé…ç½®
    - æ”¯æŒä¸åŒç²¾åº¦æ¨¡å¼ (FP32, FP16, FP8)
    - OOMä¿æŠ¤å’Œæ¢å¤æœºåˆ¶
    """
    
    def __init__(self, 
                 model: torch.nn.Module,
                 memory_monitor: MemoryMonitor,
                 device_id: int = 0):
        """
        åˆå§‹åŒ–Batch Sizeä¼˜åŒ–å™¨
        
        Args:
            model: è¦ä¼˜åŒ–çš„æ¨¡å‹
            memory_monitor: æ˜¾å­˜ç›‘æ§å™¨
            device_id: GPUè®¾å¤‡ID
        """
        self.model = model
        self.memory_monitor = memory_monitor
        self.device_id = device_id
        self.device = f'cuda:{device_id}'
        
        # æµ‹è¯•ç»“æœè®°å½•
        self.test_results = []
        self.optimization_history = []
        
        # å®‰å…¨é…ç½®
        self.safety_margin_gb = 0.5  # 500MBå®‰å…¨ä½™é‡
        self.max_test_batch_size = 64  # æœ€å¤§æµ‹è¯•batch size
        
        print(f"âœ… Batch Sizeä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ ‡æ˜¾å­˜åˆ©ç”¨ç‡: {memory_monitor.target_utilization*100:.1f}%")
        print(f"   å®‰å…¨ä½™é‡: {self.safety_margin_gb:.1f}GB")
    
    def create_sample_batch(self, 
                           batch_size: int,
                           image_size: Tuple[int, int] = (224, 224),
                           sequence_length: int = 60) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºç”¨äºæµ‹è¯•çš„æ ·æœ¬batch
        
        Args:
            batch_size: batchå¤§å°
            image_size: å›¾åƒå°ºå¯¸
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            (images, reports_ids, reports_masks)
        """
        # åˆ›å»ºæ¨¡æ‹Ÿçš„è¾“å…¥æ•°æ®
        images = torch.randn(batch_size, 3, *image_size, device=self.device)
        reports_ids = torch.randint(0, 1000, (batch_size, sequence_length), device=self.device)
        reports_masks = torch.ones(batch_size, sequence_length, device=self.device)
        
        return images, reports_ids, reports_masks
    
    def test_batch_size(self, 
                       batch_size: int,
                       precision: str = 'fp32',
                       num_test_steps: int = 3) -> Dict[str, Any]:
        """
        æµ‹è¯•ç‰¹å®šbatch sizeçš„æ˜¾å­˜ä½¿ç”¨å’Œæ€§èƒ½
        
        Args:
            batch_size: è¦æµ‹è¯•çš„batch size
            precision: ç²¾åº¦æ¨¡å¼ ('fp32', 'fp16', 'fp8')
            num_test_steps: æµ‹è¯•æ­¥æ•°
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"  ğŸ”¬ æµ‹è¯• batch_size={batch_size}, precision={precision}")
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        self.memory_monitor.clear_cache_and_reset()
        
        # è®°å½•åˆå§‹çŠ¶æ€
        initial_memory = self.memory_monitor.get_current_usage()
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = self.create_sample_batch(batch_size)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
            self.model.train()
            
            # è®°å½•æµ‹è¯•å¼€å§‹æ—¶é—´
            start_time = time.time()
            step_times = []
            memory_snapshots = []
            
            # æ‰§è¡Œæµ‹è¯•æ­¥éª¤
            for step in range(num_test_steps):
                step_start = time.time()
                
                # å‰å‘ä¼ æ’­
                if precision == 'fp16':
                    with torch.amp.autocast('cuda'):
                        output = self.model(test_data[0], test_data[1], mode='train')
                        # æ¨¡æ‹ŸæŸå¤±è®¡ç®—
                        loss = output.mean()
                elif precision == 'fp8':
                    # FP8æ”¯æŒéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œå…ˆç”¨FP16ä»£æ›¿
                    with torch.amp.autocast('cuda'):
                        output = self.model(test_data[0], test_data[1], mode='train')
                        loss = output.mean()
                else:  # fp32
                    output = self.model(test_data[0], test_data[1], mode='train')
                    loss = output.mean()
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # è®°å½•æ˜¾å­˜ä½¿ç”¨
                memory_snapshot = self.memory_monitor.get_current_usage()
                memory_snapshots.append(memory_snapshot)
                
                # æ¸…ç†æ¢¯åº¦
                self.model.zero_grad()
                
                step_time = time.time() - step_start
                step_times.append(step_time)
                
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºå®‰å…¨èŒƒå›´
                if not memory_snapshot['is_safe']:
                    print(f"    âš ï¸ è¶…å‡ºå®‰å…¨èŒƒå›´: {memory_snapshot['utilization_percent']:.1f}%")
                    break
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_time = time.time() - start_time
            avg_step_time = np.mean(step_times)
            peak_memory = max(snapshot['utilization_percent'] for snapshot in memory_snapshots)
            avg_memory = np.mean([snapshot['utilization_percent'] for snapshot in memory_snapshots])
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            samples_per_second = batch_size / avg_step_time if avg_step_time > 0 else 0
            batches_per_second = 1.0 / avg_step_time if avg_step_time > 0 else 0
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            del test_data, output, loss
            torch.cuda.empty_cache()
            
            # æœ€ç»ˆæ˜¾å­˜çŠ¶æ€
            final_memory = self.memory_monitor.get_current_usage()
            
            result = {
                'batch_size': batch_size,
                'precision': precision,
                'success': True,
                'peak_memory_percent': peak_memory,
                'avg_memory_percent': avg_memory,
                'peak_memory_gb': peak_memory * self.memory_monitor.total_memory_gb / 100,
                'avg_step_time_seconds': avg_step_time,
                'samples_per_second': samples_per_second,
                'batches_per_second': batches_per_second,
                'total_test_time': total_time,
                'num_steps_completed': len(step_times),
                'is_safe': all(snapshot['is_safe'] for snapshot in memory_snapshots),
                'memory_efficiency': (batch_size * len(step_times)) / peak_memory if peak_memory > 0 else 0,
                'initial_memory_gb': initial_memory['reserved_gb'],
                'final_memory_gb': final_memory['reserved_gb'],
                'memory_increase_gb': final_memory['reserved_gb'] - initial_memory['reserved_gb']
            }
            
            print(f"    âœ… æˆåŠŸ: {peak_memory:.1f}% å³°å€¼æ˜¾å­˜, {samples_per_second:.1f} samples/sec")
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"    âŒ OOM: batch_size={batch_size} è¶…å‡ºæ˜¾å­˜é™åˆ¶")
                
                # è®°å½•OOMäº‹ä»¶
                self.memory_monitor.record_oom_event(f"batch_size={batch_size}, precision={precision}")
                
                result = {
                    'batch_size': batch_size,
                    'precision': precision,
                    'success': False,
                    'error': 'OOM',
                    'error_message': str(e),
                    'peak_memory_percent': 100.0,  # å‡è®¾è¾¾åˆ°äº†100%
                    'is_safe': False
                }
                
                # æ¸…ç†æ˜¾å­˜
                torch.cuda.empty_cache()
            else:
                print(f"    âŒ å…¶ä»–é”™è¯¯: {e}")
                result = {
                    'batch_size': batch_size,
                    'precision': precision,
                    'success': False,
                    'error': 'other',
                    'error_message': str(e),
                    'is_safe': False
                }
        
        return result
    
    def find_optimal_batch_size(self,
                               start_size: int = 4,
                               max_size: int = 32,
                               precision: str = 'fp32',
                               step_size: int = 2) -> Tuple[int, List[Dict]]:
        """
        æ‰¾åˆ°æœ€ä¼˜çš„batch size
        
        Args:
            start_size: èµ·å§‹batch size
            max_size: æœ€å¤§batch size
            precision: ç²¾åº¦æ¨¡å¼
            step_size: æ­¥é•¿
            
        Returns:
            (æœ€ä¼˜batch_size, æµ‹è¯•ç»“æœåˆ—è¡¨)
        """
        print(f"\nğŸ” å¼€å§‹{precision}ç²¾åº¦çš„batch sizeä¼˜åŒ–...")
        print(f"   æµ‹è¯•èŒƒå›´: {start_size} - {max_size}, æ­¥é•¿: {step_size}")
        
        test_results = []
        optimal_batch_size = start_size
        best_efficiency = 0
        
        # é€æ­¥æµ‹è¯•ä¸åŒçš„batch size
        current_size = start_size
        while current_size <= max_size:
            result = self.test_batch_size(current_size, precision)
            test_results.append(result)
            
            if result['success'] and result['is_safe']:
                # è®¡ç®—æ•ˆç‡åˆ†æ•° (è€ƒè™‘batch sizeå’Œæ˜¾å­˜åˆ©ç”¨ç‡)
                efficiency = result.get('memory_efficiency', 0)
                
                if efficiency > best_efficiency:
                    best_efficiency = efficiency
                    optimal_batch_size = current_size
                
                # å¦‚æœæ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡ç›®æ ‡ï¼Œåœæ­¢æµ‹è¯•
                if result['peak_memory_percent'] > self.memory_monitor.target_utilization * 100:
                    print(f"  âš ï¸ è¾¾åˆ°ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡ï¼Œåœæ­¢æµ‹è¯•")
                    break
            else:
                # å¦‚æœå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•
                print(f"  âŒ batch_size={current_size} å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
                break
            
            current_size += step_size
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.test_results.extend(test_results)
        
        # è®°å½•ä¼˜åŒ–å†å²
        optimization_record = {
            'timestamp': time.time(),
            'precision': precision,
            'optimal_batch_size': optimal_batch_size,
            'best_efficiency': best_efficiency,
            'test_range': (start_size, min(current_size - step_size, max_size)),
            'num_tests': len(test_results),
            'successful_tests': len([r for r in test_results if r['success']])
        }
        self.optimization_history.append(optimization_record)
        
        print(f"\nâœ… {precision}ä¼˜åŒ–å®Œæˆ!")
        print(f"   æœ€ä¼˜batch size: {optimal_batch_size}")
        print(f"   æœ€ä½³æ•ˆç‡åˆ†æ•°: {best_efficiency:.2f}")
        print(f"   æˆåŠŸæµ‹è¯•: {optimization_record['successful_tests']}/{optimization_record['num_tests']}")
        
        return optimal_batch_size, test_results
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """
        è·å–ä¼˜åŒ–ç»“æœæ‘˜è¦
        
        Returns:
            ä¼˜åŒ–ç»“æœæ‘˜è¦
        """
        if not self.test_results:
            return {'status': 'no_tests_run'}
        
        successful_tests = [r for r in self.test_results if r['success']]
        
        if not successful_tests:
            return {'status': 'all_tests_failed'}
        
        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        best_test = max(successful_tests, key=lambda x: x.get('memory_efficiency', 0))
        
        summary = {
            'status': 'success',
            'optimal_batch_size': best_test['batch_size'],
            'optimal_precision': best_test['precision'],
            'peak_memory_percent': best_test['peak_memory_percent'],
            'peak_memory_gb': best_test['peak_memory_gb'],
            'samples_per_second': best_test['samples_per_second'],
            'memory_efficiency': best_test['memory_efficiency'],
            'total_tests': len(self.test_results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(self.test_results) - len(successful_tests),
            'oom_count': self.memory_monitor.oom_count,
            'optimization_history': self.optimization_history
        }
        
        return summary
    
    def print_optimization_report(self):
        """
        æ‰“å°è¯¦ç»†çš„ä¼˜åŒ–æŠ¥å‘Š
        """
        summary = self.get_optimization_summary()
        
        if summary['status'] != 'success':
            print(f"âŒ ä¼˜åŒ–å¤±è´¥: {summary['status']}")
            return
        
        print(f"\nğŸ“Š Batch Sizeä¼˜åŒ–æŠ¥å‘Š")
        print(f"=" * 50)
        print(f"âœ… æœ€ä¼˜é…ç½®:")
        print(f"   Batch Size: {summary['optimal_batch_size']}")
        print(f"   ç²¾åº¦æ¨¡å¼: {summary['optimal_precision']}")
        print(f"   å³°å€¼æ˜¾å­˜: {summary['peak_memory_percent']:.1f}% ({summary['peak_memory_gb']:.2f}GB)")
        print(f"   è®­ç»ƒé€Ÿåº¦: {summary['samples_per_second']:.1f} samples/sec")
        print(f"   å†…å­˜æ•ˆç‡: {summary['memory_efficiency']:.2f}")
        
        print(f"\nğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"   æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"   OOMæ¬¡æ•°: {summary['oom_count']}")
        
        # æ˜¾ç¤ºæ‰€æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ
        successful_tests = [r for r in self.test_results if r['success']]
        if successful_tests:
            print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
            print(f"{'Batch Size':<10} {'Memory %':<10} {'Speed':<15} {'Efficiency':<12}")
            print(f"{'-'*50}")
            for test in successful_tests:
                print(f"{test['batch_size']:<10} "
                      f"{test['peak_memory_percent']:<10.1f} "
                      f"{test['samples_per_second']:<15.1f} "
                      f"{test.get('memory_efficiency', 0):<12.2f}")


if __name__ == "__main__":
    # æµ‹è¯•Batch Sizeä¼˜åŒ–å™¨
    print("ğŸ§ª æµ‹è¯•Batch Sizeä¼˜åŒ–å™¨...")
    
    # è¿™é‡Œéœ€è¦ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleTestModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = torch.nn.Conv2d(3, 64, 3, padding=1)
            self.pool = torch.nn.AdaptiveAvgPool2d((7, 7))
            self.fc = torch.nn.Linear(64 * 7 * 7, 512)
            self.output = torch.nn.Linear(512, 1000)
        
        def forward(self, images, reports_ids=None, mode='train'):
            x = self.conv(images)
            x = self.pool(x)
            x = x.view(x.size(0), -1)
            x = self.fc(x)
            x = self.output(x)
            return x
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œç›‘æ§å™¨
    model = SimpleTestModel().cuda()
    memory_monitor = MemoryMonitor(target_utilization=0.85)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = BatchSizeOptimizer(model, memory_monitor)
    
    # è¿è¡Œä¼˜åŒ–
    optimal_size, results = optimizer.find_optimal_batch_size(
        start_size=2,
        max_size=16,
        precision='fp32',
        step_size=2
    )
    
    # æ‰“å°æŠ¥å‘Š
    optimizer.print_optimization_report()
    
    print("âœ… Batch Sizeä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆï¼")
