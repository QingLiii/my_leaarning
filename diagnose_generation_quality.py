#!/usr/bin/env python3
"""
R2Genç”Ÿæˆè´¨é‡è¯Šæ–­è„šæœ¬
æ£€æŸ¥æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡ï¼Œè¯Šæ–­BLEUåˆ†æ•°å¼‚å¸¸ä½çš„åŸå› 
"""

import sys
import torch
import numpy as np
sys.path.append('R2Gen-main')

from modules.tokenizers import Tokenizer
from modules.dataloaders import R2DataLoader
from models.r2gen import R2GenModel
import argparse

def create_args():
    """åˆ›å»ºå‚æ•°é…ç½®"""
    args = argparse.Namespace()
    
    # æ•°æ®é›†ç›¸å…³
    args.image_dir = 'datasets/iu_xray/images/'
    args.ann_path = 'datasets/iu_xray/annotation.json'
    args.dataset_name = 'iu_xray'
    args.max_seq_length = 60
    args.threshold = 3
    args.num_workers = 2
    args.batch_size = 4  # å°æ‰¹æ¬¡ç”¨äºè¯Šæ–­
    
    # æ¨¡å‹ç›¸å…³
    args.d_model = 512
    args.d_ff = 512
    args.d_vf = 2048
    args.num_heads = 8
    args.num_layers = 3
    args.dropout = 0.1
    args.logit_layers = 1
    args.bos_idx = 0
    args.eos_idx = 0
    args.drop_prob_lm = 0.5
    
    # è®°å¿†æ¨¡å—
    args.rm_num_slots = 3
    args.rm_num_heads = 8
    args.rm_d_model = 512
    
    # è§†è§‰ç‰¹å¾æå–
    args.visual_extractor = 'resnet101'
    args.visual_extractor_pretrained = True
    
    # é‡‡æ ·ç›¸å…³
    args.sample_method = 'beam_search'
    args.beam_size = 3
    args.temperature = 1.0
    args.sample_n = 1
    
    return args

def load_model(model_path, args, tokenizer):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
    model = R2GenModel(args, tokenizer)
    
    try:
        checkpoint = torch.load(model_path, map_location='cpu')
        if 'model' in checkpoint:
            model.load_state_dict(checkpoint['model'])
        else:
            model.load_state_dict(checkpoint)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    model.cuda()
    model.eval()
    return model

def diagnose_generation(model, dataloader, tokenizer, num_samples=5):
    """è¯Šæ–­ç”Ÿæˆè´¨é‡"""
    print(f"\nğŸ” å¼€å§‹è¯Šæ–­ç”Ÿæˆè´¨é‡ (æ£€æŸ¥{num_samples}ä¸ªæ ·æœ¬)...")
    
    model.eval()
    with torch.no_grad():
        for i, (images_id, images, reports_ids, reports_masks) in enumerate(dataloader):
            if i >= num_samples:
                break
                
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            
            # ç§»åŠ¨åˆ°GPU
            images = images.cuda()
            reports_ids = reports_ids.cuda()
            
            # ç”ŸæˆæŠ¥å‘Š
            try:
                generated_ids = model(images, mode='sample')
                print(f"âœ… ç”ŸæˆæˆåŠŸ")
                
                # æ£€æŸ¥ç”Ÿæˆçš„ID
                if isinstance(generated_ids, torch.Tensor):
                    gen_ids_cpu = generated_ids.cpu().numpy()
                    print(f"ğŸ“Š ç”ŸæˆIDå½¢çŠ¶: {gen_ids_cpu.shape}")
                    print(f"ğŸ“Š ç”ŸæˆIDèŒƒå›´: [{gen_ids_cpu.min()}, {gen_ids_cpu.max()}]")
                    print(f"ğŸ“Š ç”ŸæˆIDæ ·ä¾‹: {gen_ids_cpu[0][:10]}")  # å‰10ä¸ªtoken
                    
                    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                    if len(gen_ids_cpu.shape) == 2:
                        generated_text = tokenizer.decode(gen_ids_cpu[0])
                    else:
                        generated_text = "è§£ç é”™è¯¯ï¼šç»´åº¦ä¸åŒ¹é…"
                else:
                    generated_text = str(generated_ids)
                    print(f"âš ï¸ ç”Ÿæˆç»“æœä¸æ˜¯tensor: {type(generated_ids)}")
                
                print(f"ğŸ¤– ç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
                print(f"ğŸ“ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)}")
                
                # è§£ç å‚è€ƒæ–‡æœ¬
                ref_ids_cpu = reports_ids.cpu().numpy()
                ref_text = tokenizer.decode(ref_ids_cpu[0])
                print(f"ğŸ“– å‚è€ƒæ–‡æœ¬: '{ref_text}'")
                print(f"ğŸ“ å‚è€ƒæ–‡æœ¬é•¿åº¦: {len(ref_text)}")
                
                # åˆ†æé—®é¢˜
                if len(generated_text.strip()) == 0:
                    print("âŒ é—®é¢˜: ç”Ÿæˆæ–‡æœ¬ä¸ºç©ºï¼")
                elif len(generated_text.strip()) < 5:
                    print("âš ï¸ é—®é¢˜: ç”Ÿæˆæ–‡æœ¬è¿‡çŸ­ï¼")
                elif generated_text == ref_text:
                    print("ğŸ¯ å®Œç¾åŒ¹é…ï¼ˆä¸å¤ªå¯èƒ½ï¼‰")
                else:
                    print("âœ… ç”Ÿæˆæ–‡æœ¬æ­£å¸¸")
                    
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

def check_tokenizer(tokenizer):
    """æ£€æŸ¥tokenizerçŠ¶æ€"""
    print(f"\nğŸ”¤ æ£€æŸ¥TokenizerçŠ¶æ€...")
    print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    print(f"ğŸ“Š token2idxæ ·ä¾‹: {list(tokenizer.token2idx.items())[:10]}")
    print(f"ğŸ“Š idx2tokenæ ·ä¾‹: {list(tokenizer.idx2token.items())[:10]}")
    
    # æµ‹è¯•ç¼–ç è§£ç 
    test_text = "the lungs are clear"
    encoded = tokenizer(test_text)
    decoded = tokenizer.decode(encoded)
    print(f"ğŸ§ª æµ‹è¯•ç¼–ç : '{test_text}' -> {encoded}")
    print(f"ğŸ§ª æµ‹è¯•è§£ç : {encoded} -> '{decoded}'")

def main():
    print("ğŸš€ R2Genç”Ÿæˆè´¨é‡è¯Šæ–­å¼€å§‹...")
    
    # åˆ›å»ºå‚æ•°
    args = create_args()
    
    # åˆ›å»ºtokenizer
    print("ğŸ“ åˆ›å»ºtokenizer...")
    tokenizer = Tokenizer(args)
    check_tokenizer(tokenizer)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_dataloader = R2DataLoader(args, tokenizer, split='train', shuffle=False)
    
    # æ£€æŸ¥å¯ç”¨çš„æ¨¡å‹
    import os
    model_paths = [
        'results/precision_comparison/fp32/model_best.pth',
        'results/precision_comparison/fp16/model_best.pth', 
        'results/precision_comparison/fp8/model_best.pth'
    ]
    
    available_models = [path for path in model_paths if os.path.exists(path)]
    
    if not available_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒå®éªŒ")
        return
    
    # è¯Šæ–­ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    model_path = available_models[0]
    precision = model_path.split('/')[-2]
    print(f"ğŸ¯ è¯Šæ–­æ¨¡å‹: {precision}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(model_path, args, tokenizer)
    if model is None:
        return
    
    # è¯Šæ–­ç”Ÿæˆè´¨é‡
    diagnose_generation(model, train_dataloader, tokenizer, num_samples=3)
    
    print(f"\nğŸ“‹ è¯Šæ–­å®Œæˆï¼")
    print(f"å¦‚æœç”Ÿæˆæ–‡æœ¬ä¸ºç©ºæˆ–å¼‚å¸¸çŸ­ï¼Œè¿™å°±æ˜¯BLEUåˆ†æ•°å¼‚å¸¸ä½çš„åŸå› ã€‚")

if __name__ == "__main__":
    main()
