#!/usr/bin/env python3
"""
ç²¾åº¦å¯¹æ¯”å®éªŒ + æŠ¥å‘Šè´¨é‡è¯„ä¼°
å®Œæ•´çš„FP32 vs FP16 vs FP8å¯¹æ¯”ï¼ŒåŒ…å«å®šé‡æŒ‡æ ‡å’Œå®šæ€§æŠ¥å‘Šåˆ†æ
"""

import sys
import os
import argparse
import torch
import numpy as np
import json
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.append('R2Gen-main')

from modules.wandb_logger import WandBLogger
from modules.memory_monitor import MemoryMonitor
from modules.enhanced_trainer import EnhancedTrainer
from modules.report_quality_evaluator import ReportQualityEvaluator
from modules.tokenizers import Tokenizer
from modules.dataloaders import R2DataLoader
from modules.metrics import compute_scores
from modules.optimizers import build_optimizer, build_lr_scheduler
from modules.loss import compute_loss
from models.r2gen import R2GenModel


class PrecisionComparisonExperiment:
    """
    ç²¾åº¦å¯¹æ¯”å®éªŒç®¡ç†å™¨
    """
    
    def __init__(self, base_args):
        self.base_args = base_args
        self.results = {}
        self.trained_models = {}
        self.wandb_logger = WandBLogger(project_name="R2Gen-Precision-Comparison")
        self.memory_monitor = MemoryMonitor()
        
        # ç²¾åº¦é…ç½®
        self.precision_configs = {
            'fp32': {
                'mixed_precision': None,
                'description': 'Full Precision (FP32)',
                'expected_speedup': 1.0
            },
            'fp16': {
                'mixed_precision': 'fp16',
                'description': 'Half Precision (FP16)',
                'expected_speedup': 1.5
            },
            'fp8': {
                'mixed_precision': 'fp8',
                'description': 'FP8 Precision (experimental)',
                'expected_speedup': 2.0
            }
        }
        
        print(f"âœ… ç²¾åº¦å¯¹æ¯”å®éªŒåˆå§‹åŒ–å®Œæˆ")
        print(f"   å¯¹æ¯”ç²¾åº¦: {list(self.precision_configs.keys())}")
        print(f"   è®­ç»ƒepochs: {base_args.epochs}")
    
    def run_single_precision_experiment(self, precision: str) -> Dict:
        """
        è¿è¡Œå•ä¸ªç²¾åº¦çš„å®éªŒ
        
        Args:
            precision: ç²¾åº¦ç±»å‹ ('fp32', 'fp16', 'fp8')
            
        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        config = self.precision_configs[precision]
        print(f"\nğŸš€ å¼€å§‹ {precision.upper()} ç²¾åº¦å®éªŒ...")
        print(f"   æè¿°: {config['description']}")
        
        # åˆ›å»ºå®éªŒå‚æ•°
        args = self._create_experiment_args(precision, config)
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        tokenizer = Tokenizer(args)
        train_dataloader = R2DataLoader(args, tokenizer, split='train', shuffle=True)
        val_dataloader = R2DataLoader(args, tokenizer, split='val', shuffle=False)
        test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        model = R2GenModel(args, tokenizer)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = build_optimizer(args, model)
        lr_scheduler = build_lr_scheduler(args, optimizer)
        
        # åˆå§‹åŒ–WandBè¿è¡Œ
        wandb_config = {
            'precision': precision,
            'mixed_precision': config['mixed_precision'],
            'batch_size': args.batch_size,
            'epochs': args.epochs,
            'learning_rate_ve': args.lr_ve,
            'learning_rate_ed': args.lr_ed,
            'model': 'R2Gen',
            'dataset': args.dataset_name,
            'experiment_type': 'precision_comparison'
        }
        
        run_name = f"R2Gen_{precision}_precision_{args.epochs}epochs"
        self.wandb_logger.init_run(wandb_config, run_name=run_name)
        
        # åˆ›å»ºå¢å¼ºç‰ˆtrainer
        trainer = EnhancedTrainer(
            model=model,
            criterion=compute_loss,
            metric_ftns=compute_scores,
            optimizer=optimizer,
            args=args,
            lr_scheduler=lr_scheduler,
            train_dataloader=train_dataloader,
            val_dataloader=val_dataloader,
            test_dataloader=test_dataloader,
            wandb_logger=self.wandb_logger,
            enable_wandb=True
        )
        
        # è®°å½•å®éªŒå¼€å§‹
        start_time = datetime.now()
        print(f"   å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è¿è¡Œè®­ç»ƒ
        try:
            trainer.train()
            training_success = True
        except Exception as e:
            print(f"âŒ {precision} è®­ç»ƒå¤±è´¥: {e}")
            training_success = False
            return {'precision': precision, 'success': False, 'error': str(e)}
        
        # è®°å½•å®éªŒç»“æŸ
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        print(f"   ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        
        # æ”¶é›†ç»“æœ
        result = {
            'precision': precision,
            'success': training_success,
            'total_time_hours': total_time / 3600,
            'total_time_seconds': total_time,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'final_metrics': trainer.best_recorder,
            'model_path': os.path.join(args.save_dir, 'model_best.pth'),
            'config': config
        }
        
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        self.trained_models[precision] = model
        
        # ç»“æŸWandBè¿è¡Œ
        self.wandb_logger.finish()
        
        print(f"âœ… {precision.upper()} å®éªŒå®Œæˆ")
        return result
    
    def _create_experiment_args(self, precision: str, config: Dict):
        """åˆ›å»ºå®éªŒå‚æ•°"""
        import copy
        args = copy.deepcopy(self.base_args)
        
        # è®¾ç½®ç²¾åº¦ç›¸å…³å‚æ•°
        args.mixed_precision = config['mixed_precision']
        args.save_dir = f"results/precision_comparison/{precision}"
        args.experiment_name = f"R2Gen_{precision}_precision"
        
        # æ ¹æ®ç²¾åº¦è°ƒæ•´batch size (åˆ©ç”¨æ˜¾å­˜ä¼˜åŒ–ç»“æœ)
        if precision == 'fp32':
            args.batch_size = 12  # åŸºäºä¹‹å‰çš„ä¼˜åŒ–ç»“æœ
        elif precision == 'fp16':
            args.batch_size = 24  # FP16å¯ä»¥ç”¨æ›´å¤§çš„batch size
        else:  # fp8
            args.batch_size = 32  # FP8ç†è®ºä¸Šå¯ä»¥ç”¨æœ€å¤§çš„batch size
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(args.save_dir, exist_ok=True)
        
        return args
    
    def run_all_precision_experiments(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰ç²¾åº¦çš„å®éªŒ
        
        Returns:
            æ‰€æœ‰å®éªŒç»“æœ
        """
        print(f"\nğŸ¯ å¼€å§‹å®Œæ•´çš„ç²¾åº¦å¯¹æ¯”å®éªŒ")
        print(f"=" * 60)
        
        all_results = {}
        
        for precision in self.precision_configs.keys():
            try:
                result = self.run_single_precision_experiment(precision)
                all_results[precision] = result
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(all_results)
                
            except Exception as e:
                print(f"âŒ {precision} å®éªŒå¤±è´¥: {e}")
                all_results[precision] = {
                    'precision': precision,
                    'success': False,
                    'error': str(e)
                }
        
        self.results = all_results
        return all_results
    
    def evaluate_report_quality(self, num_samples: int = 5) -> Dict:
        """
        è¯„ä¼°æŠ¥å‘Šè´¨é‡
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°é‡
            
        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        if not self.trained_models:
            raise ValueError("è¯·å…ˆè¿è¡Œè®­ç»ƒå®éªŒ")
        
        print(f"\nğŸ“ å¼€å§‹æŠ¥å‘Šè´¨é‡è¯„ä¼°...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        args = self.base_args
        tokenizer = Tokenizer(args)
        test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)
        
        # æ”¶é›†æµ‹è¯•æ ·æœ¬
        test_images = []
        test_image_ids = []
        ground_truth_reports = []
        
        for i, (image_ids, images, reports_ids, reports_masks) in enumerate(test_dataloader):
            if len(test_images) >= num_samples:
                break
            
            for j in range(min(images.size(0), num_samples - len(test_images))):
                test_images.append(images[j])
                test_image_ids.append(image_ids[j])
                
                # è§£ç çœŸå®æŠ¥å‘Š
                report_tokens = reports_ids[j].cpu().numpy()
                report_text = tokenizer.decode_batch([report_tokens])[0]
                ground_truth_reports.append(report_text)
        
        print(f"   æ”¶é›†äº† {len(test_images)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # åˆ›å»ºè´¨é‡è¯„ä¼°å™¨
        evaluator = ReportQualityEvaluator(
            models_dict=self.trained_models,
            tokenizer=tokenizer
        )
        
        # ç”Ÿæˆæ ·æœ¬æŠ¥å‘Š
        sample_reports = evaluator.generate_sample_reports(
            test_images=test_images,
            test_image_ids=test_image_ids,
            ground_truth_reports=ground_truth_reports,
            num_samples=num_samples
        )
        
        # åˆ†æè´¨é‡
        quality_analysis = evaluator.analyze_report_quality()
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report_path = f"precision_comparison_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        evaluator.generate_comparison_report(report_path)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary()
        
        return {
            'sample_reports': sample_reports,
            'quality_analysis': quality_analysis,
            'report_path': report_path,
            'evaluator': evaluator
        }
    
    def _save_intermediate_results(self, results: Dict):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"precision_comparison_results_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = {}
        for precision, result in results.items():
            if result.get('success', False):
                serializable_results[precision] = {
                    'precision': result['precision'],
                    'success': result['success'],
                    'total_time_hours': result['total_time_hours'],
                    'start_time': result['start_time'],
                    'end_time': result['end_time'],
                    # æ³¨æ„ï¼šfinal_metricså¯èƒ½åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
                    'config': result['config']
                }
            else:
                serializable_results[precision] = result
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯ç”ŸæˆæŠ¥å‘Š")
            return
        
        print(f"\nğŸ“Š ç²¾åº¦å¯¹æ¯”å®éªŒæœ€ç»ˆæŠ¥å‘Š")
        print(f"=" * 60)
        
        # æˆåŠŸçš„å®éªŒ
        successful_experiments = {k: v for k, v in self.results.items() if v.get('success', False)}
        
        if not successful_experiments:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
            return
        
        print(f"âœ… æˆåŠŸå®éªŒ: {len(successful_experiments)}/{len(self.results)}")
        print(f"\nâ±ï¸ è®­ç»ƒæ—¶é—´å¯¹æ¯”:")
        
        for precision, result in successful_experiments.items():
            print(f"   {precision.upper()}: {result['total_time_hours']:.2f} å°æ—¶")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if 'fp32' in successful_experiments:
            fp32_time = successful_experiments['fp32']['total_time_hours']
            print(f"\nğŸš€ ç›¸å¯¹FP32çš„åŠ é€Ÿæ¯”:")
            
            for precision, result in successful_experiments.items():
                if precision != 'fp32':
                    speedup = fp32_time / result['total_time_hours']
                    expected = result['config']['expected_speedup']
                    print(f"   {precision.upper()}: {speedup:.2f}x (é¢„æœŸ: {expected:.1f}x)")
        
        print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®:")
        for precision, result in successful_experiments.items():
            if 'model_path' in result:
                print(f"   {precision.upper()}: {result['model_path']}")


def create_base_args():
    """åˆ›å»ºåŸºç¡€å‚æ•°"""
    class Args:
        def __init__(self):
            # æ•°æ®ç›¸å…³
            self.image_dir = 'data/iu_xray/images/'
            self.ann_path = 'data/iu_xray/annotation.json'
            self.dataset_name = 'iu_xray'
            self.max_seq_length = 60
            self.threshold = 3
            
            # è®­ç»ƒç›¸å…³
            self.epochs = 15  # ç²¾åº¦å¯¹æ¯”å®éªŒç”¨15ä¸ªepoch
            self.seed = 9223
            
            # ä¼˜åŒ–å™¨ç›¸å…³
            self.optim = 'Adam'
            self.lr_ve = 5e-5
            self.lr_ed = 1e-4
            self.weight_decay = 0
            self.amsgrad = True
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.lr_scheduler = 'StepLR'
            self.step_size = 50
            self.gamma = 0.1
            
            # æ¨¡å‹ç›¸å…³
            self.d_model = 512
            self.d_ff = 512
            self.d_vf = 2048
            self.num_heads = 8
            self.num_layers = 3
            self.dropout = 0.1
            self.logit_layers = 1
            self.bos_idx = 0
            self.eos_idx = 0
            self.pad_idx = 0
            self.use_bn = 0
            self.drop_prob_lm = 0.5
            
            # è§†è§‰ç‰¹å¾æå–
            self.visual_extractor = 'resnet101'
            self.visual_extractor_pretrained = True
            
            # è®°å¿†æ¨¡å—
            self.rm_num_slots = 3
            self.rm_num_heads = 8
            self.rm_d_model = 512
            
            # è®­ç»ƒè®¾ç½®
            self.n_gpu = 1
            self.save_period = 5
            self.monitor_mode = 'max'
            self.monitor_metric = 'BLEU_4'
            self.early_stop = 50
            self.resume = None
            
            # ä¼˜åŒ–ç›¸å…³
            self.gradient_accumulation_steps = 1
            self.log_interval = 50
    
    return Args()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='R2Genç²¾åº¦å¯¹æ¯”å®éªŒ')
    parser.add_argument('--epochs', type=int, default=15, help='è®­ç»ƒepochsæ•°')
    parser.add_argument('--quality-eval', action='store_true', help='è¿è¡ŒæŠ¥å‘Šè´¨é‡è¯„ä¼°')
    parser.add_argument('--quality-samples', type=int, default=5, help='è´¨é‡è¯„ä¼°æ ·æœ¬æ•°')
    parser.add_argument('--precision', type=str, choices=['fp32', 'fp16', 'fp8', 'all'], 
                       default='all', help='è¿è¡Œç‰¹å®šç²¾åº¦å®éªŒ')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ R2Genç²¾åº¦å¯¹æ¯”å®éªŒ")
    print(f"   è®­ç»ƒepochs: {args.epochs}")
    print(f"   ç²¾åº¦èŒƒå›´: {args.precision}")
    print(f"   è´¨é‡è¯„ä¼°: {'æ˜¯' if args.quality_eval else 'å¦'}")
    
    # åˆ›å»ºåŸºç¡€å‚æ•°
    base_args = create_base_args()
    base_args.epochs = args.epochs
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    experiment = PrecisionComparisonExperiment(base_args)
    
    # è¿è¡Œå®éªŒ
    if args.precision == 'all':
        results = experiment.run_all_precision_experiments()
    else:
        results = {args.precision: experiment.run_single_precision_experiment(args.precision)}
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    experiment.generate_final_report()
    
    # è¿è¡Œè´¨é‡è¯„ä¼°
    if args.quality_eval and any(r.get('success', False) for r in results.values()):
        print(f"\nğŸ¯ å¼€å§‹æŠ¥å‘Šè´¨é‡è¯„ä¼°...")
        quality_results = experiment.evaluate_report_quality(args.quality_samples)
        print(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {quality_results['report_path']}")
    
    print(f"\nğŸ‰ ç²¾åº¦å¯¹æ¯”å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()
