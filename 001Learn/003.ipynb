{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70abf43d",
   "metadata": {},
   "source": [
    "# R2Gen项目学习笔记 - 第二部分：数据处理与文本预处理\n",
    "\n",
    "## 上节回顾 📚\n",
    "\n",
    "在第一部分中，我们学习了：\n",
    "- R2Gen项目的整体架构\n",
    "- 主函数的执行流程  \n",
    "- 重要参数的含义\n",
    "- 深度学习的基础概念\n",
    "\n",
    "## 本节学习目标 🎯\n",
    "\n",
    "今天我们将深入学习数据处理的核心模块：\n",
    "\n",
    "1. **文本分词器 (Tokenizer)** - 如何把医学报告转换成数字\n",
    "2. **数据加载器 (DataLoader)** - 如何高效管理训练数据\n",
    "3. **图像预处理** - 如何准备医学影像数据\n",
    "4. **实际代码演示** - 动手体验数据处理过程\n",
    "\n",
    "## 为什么数据处理如此重要？ 🤔\n",
    "\n",
    "就像医生需要先整理病历资料才能诊断一样，AI模型也需要把原始的图像和文字整理成标准格式才能学习！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d68826",
   "metadata": {},
   "source": [
    "## 什么是Tokenizer（分词器）？ ✂️\n",
    "\n",
    "**分词器**是深度学习中处理文本的核心工具，它的作用是：\n",
    "\n",
    "### 🔄 主要功能\n",
    "1. **文本清洗** - 去除无用的标点符号和格式\n",
    "2. **分词切割** - 把句子切分成单词\n",
    "3. **建立词典** - 为每个单词分配唯一的数字ID\n",
    "4. **数字转换** - 把文字转换成数字序列\n",
    "\n",
    "### 🏥 医学报告的特殊性\n",
    "医学报告有其特殊性：\n",
    "- 包含专业术语（如\"心影增大\"、\"肺纹理增粗\"）\n",
    "- 格式相对固定（如\"心脏：正常大小\"）\n",
    "- 需要标准化处理\n",
    "\n",
    "### 🎯 为什么需要转换成数字？\n",
    "计算机只能理解数字，不能直接理解文字。就像：\n",
    "- \"心脏\" → 数字 ID: 156\n",
    "- \"正常\" → 数字 ID: 89  \n",
    "- \"大小\" → 数字 ID: 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0c7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🔧 分词器工作流程演示 ===\n",
      "\n",
      "📋 步骤1: 原始医学报告\n",
      "原始报告: '心脏大小正常。肺部清晰，无异常发现。'\n",
      "\n",
      "🧹 步骤2: 文本清洗\n",
      "清洗后: '心脏大小正常 . 肺部清晰无异常发现 . '\n",
      "\n",
      "✂️ 步骤3: 分词切割\n",
      "分词结果: ['心脏大小正常', '.', '肺部清晰无异常发现', '.']\n",
      "\n",
      "📖 步骤4: 建立词典\n",
      "词典示例:\n",
      "  '<unk>' → 0\n",
      "  '心脏' → 1\n",
      "  '大小' → 2\n",
      "  '正常' → 3\n",
      "  '肺部' → 4\n",
      "  '清晰' → 5\n",
      "  '无' → 6\n",
      "  '异常' → 7\n",
      "  '发现' → 8\n",
      "  '.' → 9\n",
      "\n",
      "🔢 步骤5: 转换为数字序列\n",
      "最终数字序列: [0, 9, 0, 9]\n",
      "对应关系:\n",
      "  '心脏大小正常' → 0\n",
      "  '.' → 9\n",
      "  '肺部清晰无异常发现' → 0\n",
      "  '.' → 9\n",
      "\n",
      "💡 这样计算机就能'理解'医学报告了！\n"
     ]
    }
   ],
   "source": [
    "# 让我们用简单的例子理解Tokenizer的工作原理\n",
    "\n",
    "def simple_tokenizer_demo():\n",
    "    \"\"\"\n",
    "    演示分词器的基本工作流程\n",
    "    这是一个简化版本，帮助理解核心概念\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🔧 分词器工作流程演示 ===\\n\")\n",
    "    \n",
    "    # 步骤1: 原始医学报告\n",
    "    print(\"📋 步骤1: 原始医学报告\")\n",
    "    raw_report = \"心脏大小正常。肺部清晰，无异常发现。\"\n",
    "    print(f\"原始报告: '{raw_report}'\\n\")\n",
    "    \n",
    "    # 步骤2: 文本清洗\n",
    "    print(\"🧹 步骤2: 文本清洗\")\n",
    "    # 去除标点符号，转小写，分句\n",
    "    cleaned = raw_report.replace(\"。\", \" . \").replace(\"，\", \"\").lower()\n",
    "    print(f\"清洗后: '{cleaned}'\\n\")\n",
    "    \n",
    "    # 步骤3: 分词\n",
    "    print(\"✂️ 步骤3: 分词切割\")\n",
    "    tokens = cleaned.split()\n",
    "    print(f\"分词结果: {tokens}\\n\")\n",
    "    \n",
    "    # 步骤4: 建立词典（简化版）\n",
    "    print(\"📖 步骤4: 建立词典\")\n",
    "    # 实际中会统计所有训练数据中的词汇\n",
    "    vocab = {\n",
    "        '<unk>': 0,  # 未知词\n",
    "        '心脏': 1,\n",
    "        '大小': 2, \n",
    "        '正常': 3,\n",
    "        '肺部': 4,\n",
    "        '清晰': 5,\n",
    "        '无': 6,\n",
    "        '异常': 7,\n",
    "        '发现': 8,\n",
    "        '.': 9\n",
    "    }\n",
    "    print(\"词典示例:\")\n",
    "    for word, idx in vocab.items():\n",
    "        print(f\"  '{word}' → {idx}\")\n",
    "    print()\n",
    "    \n",
    "    # 步骤5: 转换为数字序列\n",
    "    print(\"🔢 步骤5: 转换为数字序列\")\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            token_ids.append(vocab[token])\n",
    "        else:\n",
    "            token_ids.append(vocab['<unk>'])  # 未知词用0表示\n",
    "    \n",
    "    print(f\"最终数字序列: {token_ids}\")\n",
    "    print(\"对应关系:\")\n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        print(f\"  '{token}' → {token_id}\")\n",
    "    \n",
    "    print(\"\\n💡 这样计算机就能'理解'医学报告了！\")\n",
    "\n",
    "# 运行演示\n",
    "simple_tokenizer_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e386a6f",
   "metadata": {},
   "source": [
    "## 数据加载器 (DataLoader) 解析 📦\n",
    "\n",
    "**数据加载器**是训练过程中的\"后勤部门\"，负责：\n",
    "- 🔄 批量加载数据\n",
    "- 🖼️ 图像预处理  \n",
    "- 📝 文本预处理\n",
    "- 🔀 数据打乱和采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2c1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🏭 DataLoader: 训练数据的'生产流水线' ===\n",
      "\n",
      "🎯 核心作用:\n",
      "就像工厂的流水线一样，DataLoader负责:\n",
      "• 从仓库（数据集）中取出原材料（原始数据）\n",
      "• 按照标准流程加工（预处理）\n",
      "• 打包成标准规格（批量处理）\n",
      "• 源源不断供应给生产线（模型训练）\n",
      "\n",
      "📊 批量处理 (Batch Processing):\n",
      "• 不是一张一张图片处理，而是一次处理16张（batch_size=16）\n",
      "• 就像医生不是一个一个看病人，而是安排一批病人同时检查\n",
      "• 大大提高了处理效率\n",
      "\n",
      "🖼️ 图像预处理流程:\n",
      "训练时:\n",
      "  1. Resize(256) - 调整图片大小到256x256\n",
      "  2. RandomCrop(224) - 随机裁剪到224x224（数据增强）\n",
      "  3. RandomHorizontalFlip() - 随机水平翻转（数据增强）\n",
      "  4. ToTensor() - 转换为张量格式\n",
      "  5. Normalize() - 标准化像素值\n",
      "\n",
      "测试时:\n",
      "  1. Resize(224, 224) - 直接调整到224x224（不随机）\n",
      "  2. ToTensor() - 转换为张量\n",
      "  3. Normalize() - 标准化\n",
      "\n",
      "🧮 为什么要标准化？\n",
      "• 原始像素值范围: 0-255\n",
      "• 标准化后范围: 大约-2到2之间\n",
      "• 这样神经网络训练更稳定，收敛更快\n",
      "• 就像把不同单位的数据（米、厘米、毫米）统一成一个标准\n",
      "\n",
      "🔀 数据打乱 (Shuffle):\n",
      "• 训练时: shuffle=True（打乱数据顺序）\n",
      "• 测试时: shuffle=False（保持固定顺序）\n",
      "• 为什么要打乱？避免模型记住数据顺序而不是真正学习规律\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 工作原理详解\n",
    "\n",
    "def explain_dataloader():\n",
    "    \"\"\"\n",
    "    解释数据加载器的工作原理和重要性\n",
    "    用生活化的比喻帮助理解\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🏭 DataLoader: 训练数据的'生产流水线' ===\\n\")\n",
    "    \n",
    "    print(\"🎯 核心作用:\")\n",
    "    print(\"就像工厂的流水线一样，DataLoader负责:\")\n",
    "    print(\"• 从仓库（数据集）中取出原材料（原始数据）\")\n",
    "    print(\"• 按照标准流程加工（预处理）\")\n",
    "    print(\"• 打包成标准规格（批量处理）\")\n",
    "    print(\"• 源源不断供应给生产线（模型训练）\\n\")\n",
    "    \n",
    "    print(\"📊 批量处理 (Batch Processing):\")\n",
    "    print(\"• 不是一张一张图片处理，而是一次处理16张（batch_size=16）\")\n",
    "    print(\"• 就像医生不是一个一个看病人，而是安排一批病人同时检查\")\n",
    "    print(\"• 大大提高了处理效率\\n\")\n",
    "    \n",
    "    print(\"🖼️ 图像预处理流程:\")\n",
    "    print(\"训练时:\")\n",
    "    print(\"  1. Resize(256) - 调整图片大小到256x256\")\n",
    "    print(\"  2. RandomCrop(224) - 随机裁剪到224x224（数据增强）\")\n",
    "    print(\"  3. RandomHorizontalFlip() - 随机水平翻转（数据增强）\") \n",
    "    print(\"  4. ToTensor() - 转换为张量格式\")\n",
    "    print(\"  5. Normalize() - 标准化像素值\")\n",
    "    print()\n",
    "    print(\"测试时:\")\n",
    "    print(\"  1. Resize(224, 224) - 直接调整到224x224（不随机）\")\n",
    "    print(\"  2. ToTensor() - 转换为张量\")\n",
    "    print(\"  3. Normalize() - 标准化\\n\")\n",
    "    \n",
    "    print(\"🧮 为什么要标准化？\")\n",
    "    print(\"• 原始像素值范围: 0-255\")  \n",
    "    print(\"• 标准化后范围: 大约-2到2之间\")\n",
    "    print(\"• 这样神经网络训练更稳定，收敛更快\")\n",
    "    print(\"• 就像把不同单位的数据（米、厘米、毫米）统一成一个标准\\n\")\n",
    "    \n",
    "    print(\"🔀 数据打乱 (Shuffle):\")\n",
    "    print(\"• 训练时: shuffle=True（打乱数据顺序）\")\n",
    "    print(\"• 测试时: shuffle=False（保持固定顺序）\")\n",
    "    print(\"• 为什么要打乱？避免模型记住数据顺序而不是真正学习规律\")\n",
    "\n",
    "# 运行解释\n",
    "explain_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb250bb",
   "metadata": {},
   "source": [
    "## 实际数据处理演示 🛠️\n",
    "\n",
    "让我们动手体验一下数据处理的完整流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae82bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🔄 完整数据处理流程模拟 ===\n",
      "\n",
      "📁 步骤1: 原始数据\n",
      "图像路径: datasets/iu_xray/images/CXR1_1_IM-0001/1.png\n",
      "医学报告: '心脏大小正常。双肺清晰，未见明显异常。肋骨完整。'\n",
      "\n",
      "✂️ 步骤2: 文本预处理\n",
      "清洗后报告: '心脏大小正常 双肺清晰未见明显异常 肋骨完整 .'\n",
      "分词结果: ['心脏大小正常', '双肺清晰未见明显异常', '肋骨完整', '.']\n",
      "转换为ID: [0, 0, 0, 12]\n",
      "\n",
      "🖼️ 步骤3: 图像预处理\n",
      "假设原始图像大小: 512x512 像素\n",
      "预处理步骤:\n",
      "  1. 调整大小: 512x512 → 256x256\n",
      "  2. 随机裁剪: 256x256 → 224x224\n",
      "  3. 随机翻转: 50%概率水平翻转\n",
      "  4. 转换张量: HWC格式 → CHW格式\n",
      "  5. 标准化: [0,255] → [-2,2]范围\n",
      "\n",
      "📦 步骤4: 批量组装\n",
      "假设batch_size=16，将16个样本组装成一个批次:\n",
      "• 图像批次形状: [16, 3, 224, 224]\n",
      "  - 16: 批次大小\n",
      "  - 3: RGB三通道\n",
      "  - 224x224: 图像尺寸\n",
      "• 文本批次形状: [16, max_length]\n",
      "  - 16: 批次大小\n",
      "  - max_length: 最长报告的长度（短的会填充）\n",
      "\n",
      "🎯 最终输出:\n",
      "模型接收到的数据格式:\n",
      "• images: torch.Tensor[16, 3, 224, 224]\n",
      "• reports: torch.LongTensor[16, 60]\n",
      "• masks: torch.FloatTensor[16, 60]\n",
      "\n",
      "✅ 数据准备完成，可以开始训练！\n"
     ]
    }
   ],
   "source": [
    "# 模拟完整的数据处理流程\n",
    "\n",
    "def data_processing_simulation():\n",
    "    \"\"\"\n",
    "    模拟R2Gen中的完整数据处理流程\n",
    "    让大家体验从原始数据到训练数据的转换过程\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🔄 完整数据处理流程模拟 ===\\n\")\n",
    "    \n",
    "    # 模拟原始数据\n",
    "    print(\"📁 步骤1: 原始数据\")\n",
    "    raw_data = {\n",
    "        'image_path': 'datasets/iu_xray/images/CXR1_1_IM-0001/1.png',\n",
    "        'report': '心脏大小正常。双肺清晰，未见明显异常。肋骨完整。'\n",
    "    }\n",
    "    print(f\"图像路径: {raw_data['image_path']}\")\n",
    "    print(f\"医学报告: '{raw_data['report']}'\\n\")\n",
    "    \n",
    "    # 步骤2: 文本预处理\n",
    "    print(\"✂️ 步骤2: 文本预处理\")\n",
    "    \n",
    "    # 简化的清洗函数\n",
    "    def simple_clean(text):\n",
    "        # 去除标点，转小写，添加句点\n",
    "        import re\n",
    "        text = text.replace('。', ' .').replace('，', '')\n",
    "        text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text)\n",
    "        return text.lower().strip() + ' .'\n",
    "    \n",
    "    cleaned_report = simple_clean(raw_data['report'])\n",
    "    print(f\"清洗后报告: '{cleaned_report}'\")\n",
    "    \n",
    "    # 分词\n",
    "    tokens = cleaned_report.split()\n",
    "    print(f\"分词结果: {tokens}\")\n",
    "    \n",
    "    # 转换为ID（简化的词典）\n",
    "    vocab = {'心脏': 1, '大小': 2, '正常': 3, '双肺': 4, '清晰': 5, \n",
    "             '未': 6, '见': 7, '明显': 8, '异常': 9, '肋骨': 10, \n",
    "             '完整': 11, '.': 12, '<unk>': 0}\n",
    "    \n",
    "    token_ids = [vocab.get(token, 0) for token in tokens]\n",
    "    print(f\"转换为ID: {token_ids}\\n\")\n",
    "    \n",
    "    # 步骤3: 图像预处理模拟\n",
    "    print(\"🖼️ 步骤3: 图像预处理\")\n",
    "    print(\"假设原始图像大小: 512x512 像素\")\n",
    "    print(\"预处理步骤:\")\n",
    "    print(\"  1. 调整大小: 512x512 → 256x256\")\n",
    "    print(\"  2. 随机裁剪: 256x256 → 224x224\")\n",
    "    print(\"  3. 随机翻转: 50%概率水平翻转\")\n",
    "    print(\"  4. 转换张量: HWC格式 → CHW格式\")\n",
    "    print(\"  5. 标准化: [0,255] → [-2,2]范围\\n\")\n",
    "    \n",
    "    # 步骤4: 批量组装\n",
    "    print(\"📦 步骤4: 批量组装\")\n",
    "    print(\"假设batch_size=16，将16个样本组装成一个批次:\")\n",
    "    print(\"• 图像批次形状: [16, 3, 224, 224]\")\n",
    "    print(\"  - 16: 批次大小\")\n",
    "    print(\"  - 3: RGB三通道\") \n",
    "    print(\"  - 224x224: 图像尺寸\")\n",
    "    print(\"• 文本批次形状: [16, max_length]\")\n",
    "    print(\"  - 16: 批次大小\")\n",
    "    print(\"  - max_length: 最长报告的长度（短的会填充）\\n\")\n",
    "    \n",
    "    print(\"🎯 最终输出:\")\n",
    "    print(\"模型接收到的数据格式:\")\n",
    "    print(\"• images: torch.Tensor[16, 3, 224, 224]\")\n",
    "    print(\"• reports: torch.LongTensor[16, 60]\")\n",
    "    print(\"• masks: torch.FloatTensor[16, 60]\")\n",
    "    print(\"\\n✅ 数据准备完成，可以开始训练！\")\n",
    "\n",
    "# 运行模拟\n",
    "data_processing_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51499366",
   "metadata": {},
   "source": [
    "## 本节总结与下节预告 📝\n",
    "\n",
    "### 🎯 今天我们学到了什么？\n",
    "\n",
    "1. **Tokenizer分词器**\n",
    "   - 文本清洗和标准化\n",
    "   - 词汇表构建和管理\n",
    "   - 文字与数字的双向转换\n",
    "\n",
    "2. **DataLoader数据加载器**\n",
    "   - 批量数据处理\n",
    "   - 图像预处理流程\n",
    "   - 高效的数据管道设计\n",
    "\n",
    "3. **数据增强技术**\n",
    "   - 提高数据多样性的方法\n",
    "   - 医学影像的特殊考虑\n",
    "   - 防止过拟合的策略\n",
    "\n",
    "### 🧠 关键理解点\n",
    "\n",
    "- **数据预处理**是深度学习成功的基础\n",
    "- **标准化**让模型训练更稳定\n",
    "- **批量处理**大大提高计算效率\n",
    "- **数据增强**提升模型泛化能力\n",
    "\n",
    "### 🚀 下节预告\n",
    "\n",
    "下一节我们将学习R2Gen的核心架构：\n",
    "1. **视觉特征提取器** - CNN如何\"看懂\"医学影像\n",
    "2. **编码器-解码器结构** - 如何从图像生成文本\n",
    "3. **注意力机制** - 让AI学会\"重点关注\"\n",
    "\n",
    "### 💪 练习建议\n",
    "\n",
    "1. 回顾数据处理的每个步骤，理解其必要性\n",
    "2. 思考：为什么要将文字转换成数字？\n",
    "3. 想象一下：如果你是AI，你希望接收什么样格式的数据？\n",
    "\n",
    "---\n",
    "**记住：好的数据预处理是模型成功的一半！** 🎯✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f02818",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# R2Gen项目学习笔记 - 第二部分：数据处理与文本预处理\n",
    "\n",
    "## 上节回顾 📚\n",
    "\n",
    "在第一部分中，我们学习了：\n",
    "- R2Gen项目的整体架构\n",
    "- 主函数的执行流程  \n",
    "- 重要参数的含义\n",
    "- 深度学习的基础概念\n",
    "\n",
    "## 本节学习目标 🎯\n",
    "\n",
    "今天我们将深入学习数据处理的核心模块：\n",
    "\n",
    "1. **文本分词器 (Tokenizer)** - 如何把医学报告转换成数字\n",
    "2. **数据加载器 (DataLoader)** - 如何高效管理训练数据\n",
    "3. **图像预处理** - 如何准备医学影像数据\n",
    "4. **实际代码演示** - 动手体验数据处理过程\n",
    "\n",
    "## 为什么数据处理如此重要？ 🤔\n",
    "\n",
    "就像医生需要先整理病历资料才能诊断一样，AI模型也需要把原始的图像和文字整理成标准格式才能学习！\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 什么是Tokenizer（分词器）？ ✂️\n",
    "\n",
    "**分词器**是深度学习中处理文本的核心工具，它的作用是：\n",
    "\n",
    "### 🔄 主要功能\n",
    "1. **文本清洗** - 去除无用的标点符号和格式\n",
    "2. **分词切割** - 把句子切分成单词\n",
    "3. **建立词典** - 为每个单词分配唯一的数字ID\n",
    "4. **数字转换** - 把文字转换成数字序列\n",
    "\n",
    "### 🏥 医学报告的特殊性\n",
    "医学报告有其特殊性：\n",
    "- 包含专业术语（如\"心影增大\"、\"肺纹理增粗\"）\n",
    "- 格式相对固定（如\"心脏：正常大小\"）\n",
    "- 需要标准化处理\n",
    "\n",
    "### 🎯 为什么需要转换成数字？\n",
    "计算机只能理解数字，不能直接理解文字。就像：\n",
    "- \"心脏\" → 数字 ID: 156\n",
    "- \"正常\" → 数字 ID: 89  \n",
    "- \"大小\" → 数字 ID: 203\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# 让我们用简单的例子理解Tokenizer的工作原理\n",
    "\n",
    "def simple_tokenizer_demo():\n",
    "    \"\"\"\n",
    "    演示分词器的基本工作流程\n",
    "    这是一个简化版本，帮助理解核心概念\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🔧 分词器工作流程演示 ===\\n\")\n",
    "    \n",
    "    # 步骤1: 原始医学报告\n",
    "    print(\"📋 步骤1: 原始医学报告\")\n",
    "    raw_report = \"心脏大小正常。肺部清晰，无异常发现。\"\n",
    "    print(f\"原始报告: '{raw_report}'\\n\")\n",
    "    \n",
    "    # 步骤2: 文本清洗\n",
    "    print(\"🧹 步骤2: 文本清洗\")\n",
    "    # 去除标点符号，转小写，分句\n",
    "    cleaned = raw_report.replace(\"。\", \" . \").replace(\"，\", \"\").lower()\n",
    "    print(f\"清洗后: '{cleaned}'\\n\")\n",
    "    \n",
    "    # 步骤3: 分词\n",
    "    print(\"✂️ 步骤3: 分词切割\")\n",
    "    tokens = cleaned.split()\n",
    "    print(f\"分词结果: {tokens}\\n\")\n",
    "    \n",
    "    # 步骤4: 建立词典（简化版）\n",
    "    print(\"📖 步骤4: 建立词典\")\n",
    "    # 实际中会统计所有训练数据中的词汇\n",
    "    vocab = {\n",
    "        '<unk>': 0,  # 未知词\n",
    "        '心脏': 1,\n",
    "        '大小': 2, \n",
    "        '正常': 3,\n",
    "        '肺部': 4,\n",
    "        '清晰': 5,\n",
    "        '无': 6,\n",
    "        '异常': 7,\n",
    "        '发现': 8,\n",
    "        '.': 9\n",
    "    }\n",
    "    print(\"词典示例:\")\n",
    "    for word, idx in vocab.items():\n",
    "        print(f\"  '{word}' → {idx}\")\n",
    "    print()\n",
    "    \n",
    "    # 步骤5: 转换为数字序列\n",
    "    print(\"🔢 步骤5: 转换为数字序列\")\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            token_ids.append(vocab[token])\n",
    "        else:\n",
    "            token_ids.append(vocab['<unk>'])  # 未知词用0表示\n",
    "    \n",
    "    print(f\"最终数字序列: {token_ids}\")\n",
    "    print(\"对应关系:\")\n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        print(f\"  '{token}' → {token_id}\")\n",
    "    \n",
    "    print(\"\\n💡 这样计算机就能'理解'医学报告了！\")\n",
    "\n",
    "# 运行演示\n",
    "simple_tokenizer_demo()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## R2Gen中的Tokenizer详解 🔍\n",
    "\n",
    "让我们看看R2Gen项目中真正的Tokenizer是如何工作的：\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# R2Gen Tokenizer 的核心功能分析\n",
    "\n",
    "def analyze_r2gen_tokenizer():\n",
    "    \"\"\"\n",
    "    分析R2Gen项目中Tokenizer的具体实现\n",
    "    理解专业的文本处理流程\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🏥 R2Gen Tokenizer 核心功能 ===\\n\")\n",
    "    \n",
    "    # 功能1: 文本清洗\n",
    "    print(\"🧼 功能1: 专业的文本清洗\")\n",
    "    print(\"• 去除多余的标点符号和数字编号\")\n",
    "    print(\"• 处理换行符和多余空格\")\n",
    "    print(\"• 统一转换为小写\")\n",
    "    print(\"• 特殊处理医学报告的格式\")\n",
    "    \n",
    "    # 示例清洗过程\n",
    "    original_text = \"1. Heart size is normal.. 2. Lungs are clear.\"\n",
    "    print(f\"\\n原始文本: '{original_text}'\")\n",
    "    \n",
    "    # 模拟清洗步骤\n",
    "    step1 = original_text.replace('..', '.').replace('1. ', '').replace('. 2. ', '. ')\n",
    "    step2 = step1.strip().lower()\n",
    "    # 去除标点符号（简化版）\n",
    "    import re\n",
    "    step3 = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', step2.replace('\"', ''))\n",
    "    final_clean = step3 + ' .'\n",
    "    \n",
    "    print(f\"清洗后: '{final_clean}'\\n\")\n",
    "    \n",
    "    # 功能2: 词汇表构建\n",
    "    print(\"📚 功能2: 智能词汇表构建\")\n",
    "    print(\"• 统计所有训练数据中的词汇出现频率\")\n",
    "    print(\"• 只保留出现次数 >= threshold 的词汇\")\n",
    "    print(\"• 自动过滤掉罕见词汇，提高训练效率\")\n",
    "    print(\"• 添加特殊标记：<unk>（未知词）\\n\")\n",
    "    \n",
    "    # 功能3: 编码解码\n",
    "    print(\"🔄 功能3: 双向编码解码\")\n",
    "    print(\"• encode: 文字 → 数字序列\")\n",
    "    print(\"• decode: 数字序列 → 文字\")\n",
    "    print(\"• 支持批量处理多个报告\")\n",
    "    print(\"• 自动添加起始和结束标记\\n\")\n",
    "    \n",
    "    # 功能4: 特殊处理\n",
    "    print(\"⚡ 功能4: 智能特殊处理\")\n",
    "    print(\"• 处理未知词汇：遇到词典中没有的词用<unk>代替\")\n",
    "    print(\"• 长度控制：超长报告会被截断\")\n",
    "    print(\"• 填充处理：短报告会被填充到固定长度\")\n",
    "    \n",
    "    print(\"\\n🎯 核心优势：\")\n",
    "    print(\"• 专门针对医学文本优化\")\n",
    "    print(\"• 处理速度快，内存占用少\")\n",
    "    print(\"• 可以处理IU X-Ray和MIMIC-CXR两种不同数据集\")\n",
    "\n",
    "# 运行分析\n",
    "analyze_r2gen_tokenizer()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 数据加载器 (DataLoader) 解析 📦\n",
    "\n",
    "**数据加载器**是训练过程中的\"后勤部门\"，负责：\n",
    "- 🔄 批量加载数据\n",
    "- 🖼️ 图像预处理  \n",
    "- 📝 文本预处理\n",
    "- 🔀 数据打乱和采样\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# DataLoader 工作原理详解\n",
    "\n",
    "def explain_dataloader():\n",
    "    \"\"\"\n",
    "    解释数据加载器的工作原理和重要性\n",
    "    用生活化的比喻帮助理解\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🏭 DataLoader: 训练数据的'生产流水线' ===\\n\")\n",
    "    \n",
    "    print(\"🎯 核心作用:\")\n",
    "    print(\"就像工厂的流水线一样，DataLoader负责:\")\n",
    "    print(\"• 从仓库（数据集）中取出原材料（原始数据）\")\n",
    "    print(\"• 按照标准流程加工（预处理）\")\n",
    "    print(\"• 打包成标准规格（批量处理）\")\n",
    "    print(\"• 源源不断供应给生产线（模型训练）\\n\")\n",
    "    \n",
    "    print(\"📊 批量处理 (Batch Processing):\")\n",
    "    print(\"• 不是一张一张图片处理，而是一次处理16张（batch_size=16）\")\n",
    "    print(\"• 就像医生不是一个一个看病人，而是安排一批病人同时检查\")\n",
    "    print(\"• 大大提高了处理效率\\n\")\n",
    "    \n",
    "    print(\"🖼️ 图像预处理流程:\")\n",
    "    print(\"训练时:\")\n",
    "    print(\"  1. Resize(256) - 调整图片大小到256x256\")\n",
    "    print(\"  2. RandomCrop(224) - 随机裁剪到224x224（数据增强）\")\n",
    "    print(\"  3. RandomHorizontalFlip() - 随机水平翻转（数据增强）\") \n",
    "    print(\"  4. ToTensor() - 转换为张量格式\")\n",
    "    print(\"  5. Normalize() - 标准化像素值\")\n",
    "    print()\n",
    "    print(\"测试时:\")\n",
    "    print(\"  1. Resize(224, 224) - 直接调整到224x224（不随机）\")\n",
    "    print(\"  2. ToTensor() - 转换为张量\")\n",
    "    print(\"  3. Normalize() - 标准化\\n\")\n",
    "    \n",
    "    print(\"🧮 为什么要标准化？\")\n",
    "    print(\"• 原始像素值范围: 0-255\")  \n",
    "    print(\"• 标准化后范围: 大约-2到2之间\")\n",
    "    print(\"• 这样神经网络训练更稳定，收敛更快\")\n",
    "    print(\"• 就像把不同单位的数据（米、厘米、毫米）统一成一个标准\\n\")\n",
    "    \n",
    "    print(\"🔀 数据打乱 (Shuffle):\")\n",
    "    print(\"• 训练时: shuffle=True（打乱数据顺序）\")\n",
    "    print(\"• 测试时: shuffle=False（保持固定顺序）\")\n",
    "    print(\"• 为什么要打乱？避免模型记住数据顺序而不是真正学习规律\")\n",
    "\n",
    "# 运行解释\n",
    "explain_dataloader()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 实际数据处理演示 🛠️\n",
    "\n",
    "让我们动手体验一下数据处理的完整流程：\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# 模拟完整的数据处理流程\n",
    "\n",
    "def data_processing_simulation():\n",
    "    \"\"\"\n",
    "    模拟R2Gen中的完整数据处理流程\n",
    "    让大家体验从原始数据到训练数据的转换过程\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🔄 完整数据处理流程模拟 ===\\n\")\n",
    "    \n",
    "    # 模拟原始数据\n",
    "    print(\"📁 步骤1: 原始数据\")\n",
    "    raw_data = {\n",
    "        'image_path': 'datasets/iu_xray/images/CXR1_1_IM-0001/1.png',\n",
    "        'report': '心脏大小正常。双肺清晰，未见明显异常。肋骨完整。'\n",
    "    }\n",
    "    print(f\"图像路径: {raw_data['image_path']}\")\n",
    "    print(f\"医学报告: '{raw_data['report']}'\\n\")\n",
    "    \n",
    "    # 步骤2: 文本预处理\n",
    "    print(\"✂️ 步骤2: 文本预处理\")\n",
    "    \n",
    "    # 简化的清洗函数\n",
    "    def simple_clean(text):\n",
    "        # 去除标点，转小写，添加句点\n",
    "        import re\n",
    "        text = text.replace('。', ' .').replace('，', '')\n",
    "        text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text)\n",
    "        return text.lower().strip() + ' .'\n",
    "    \n",
    "    cleaned_report = simple_clean(raw_data['report'])\n",
    "    print(f\"清洗后报告: '{cleaned_report}'\")\n",
    "    \n",
    "    # 分词\n",
    "    tokens = cleaned_report.split()\n",
    "    print(f\"分词结果: {tokens}\")\n",
    "    \n",
    "    # 转换为ID（简化的词典）\n",
    "    vocab = {'心脏': 1, '大小': 2, '正常': 3, '双肺': 4, '清晰': 5, \n",
    "             '未': 6, '见': 7, '明显': 8, '异常': 9, '肋骨': 10, \n",
    "             '完整': 11, '.': 12, '<unk>': 0}\n",
    "    \n",
    "    token_ids = [vocab.get(token, 0) for token in tokens]\n",
    "    print(f\"转换为ID: {token_ids}\\n\")\n",
    "    \n",
    "    # 步骤3: 图像预处理模拟\n",
    "    print(\"🖼️ 步骤3: 图像预处理\")\n",
    "    print(\"假设原始图像大小: 512x512 像素\")\n",
    "    print(\"预处理步骤:\")\n",
    "    print(\"  1. 调整大小: 512x512 → 256x256\")\n",
    "    print(\"  2. 随机裁剪: 256x256 → 224x224\")\n",
    "    print(\"  3. 随机翻转: 50%概率水平翻转\")\n",
    "    print(\"  4. 转换张量: HWC格式 → CHW格式\")\n",
    "    print(\"  5. 标准化: [0,255] → [-2,2]范围\\n\")\n",
    "    \n",
    "    # 步骤4: 批量组装\n",
    "    print(\"📦 步骤4: 批量组装\")\n",
    "    print(\"假设batch_size=16，将16个样本组装成一个批次:\")\n",
    "    print(\"• 图像批次形状: [16, 3, 224, 224]\")\n",
    "    print(\"  - 16: 批次大小\")\n",
    "    print(\"  - 3: RGB三通道\") \n",
    "    print(\"  - 224x224: 图像尺寸\")\n",
    "    print(\"• 文本批次形状: [16, max_length]\")\n",
    "    print(\"  - 16: 批次大小\")\n",
    "    print(\"  - max_length: 最长报告的长度（短的会填充）\\n\")\n",
    "    \n",
    "    print(\"🎯 最终输出:\")\n",
    "    print(\"模型接收到的数据格式:\")\n",
    "    print(\"• images: torch.Tensor[16, 3, 224, 224]\")\n",
    "    print(\"• reports: torch.LongTensor[16, 60]\")\n",
    "    print(\"• masks: torch.FloatTensor[16, 60]\")\n",
    "    print(\"\\n✅ 数据准备完成，可以开始训练！\")\n",
    "\n",
    "# 运行模拟\n",
    "data_processing_simulation()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 数据增强技术 🔄\n",
    "\n",
    "**数据增强**是提高模型性能的重要技术：\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# 数据增强技术详解\n",
    "\n",
    "def explain_data_augmentation():\n",
    "    \"\"\"\n",
    "    解释数据增强技术及其在医学影像中的应用\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 🔄 数据增强: 让AI见多识广 ===\\n\")\n",
    "    \n",
    "    print(\"🤔 什么是数据增强？\")\n",
    "    print(\"通过对原始数据进行微小的变换，创造更多训练样本\")\n",
    "    print(\"就像让医学生看同一个X光片的不同角度和光照条件\\n\")\n",
    "    \n",
    "    print(\"🏥 医学影像中的数据增强技术:\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. 🔧 RandomCrop (随机裁剪)\")\n",
    "    print(\"   • 从256x256的图像中随机裁剪224x224区域\")\n",
    "    print(\"   • 让模型学会关注图像的不同部分\")\n",
    "    print(\"   • 提高模型的泛化能力\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. 🔄 RandomHorizontalFlip (随机水平翻转)\")\n",
    "    print(\"   • 50%的概率将图像水平翻转\")\n",
    "    print(\"   • 对于胸部X光片，左右翻转通常不影响诊断\")\n",
    "    print(\"   • 有效增加数据的多样性\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. 📏 Resize (尺寸调整)\")\n",
    "    print(\"   • 将不同尺寸的医学影像统一到固定大小\")\n",
    "    print(\"   • 确保模型输入的一致性\")\n",
    "    print(\"   • 训练时: 256x256，测试时: 224x224\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. 🎨 Normalize (标准化)\")\n",
    "    print(\"   • 使用ImageNet的均值和标准差\")\n",
    "    print(\"   • mean=[0.485, 0.456, 0.406]\")\n",
    "    print(\"   • std=[0.229, 0.224, 0.225]\")\n",
    "    print(\"   • 这样可以利用在ImageNet上预训练的模型\")\n",
    "    print()\n",
    "    \n",
    "    print(\"⚠️ 医学影像数据增强的注意事项:\")\n",
    "    print(\"• 不能随意旋转（可能改变病理特征）\")\n",
    "    print(\"• 不能改变像素值分布（可能影响诊断）\")\n",
    "    print(\"• 保持医学影像的真实性和可解释性\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💡 为什么数据增强有效？\")\n",
    "    print(\"• 增加训练数据的多样性\")\n",
    "    print(\"• 提高模型的泛化能力\") \n",
    "    print(\"• 减少过拟合现象\")\n",
    "    print(\"• 让模型更鲁棒（robust）\")\n",
    "\n",
    "# 运行解释\n",
    "explain_data_augmentation()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 本节总结与下节预告 📝\n",
    "\n",
    "### 🎯 今天我们学到了什么？\n",
    "\n",
    "1. **Tokenizer分词器**\n",
    "   - 文本清洗和标准化\n",
    "   - 词汇表构建和管理\n",
    "   - 文字与数字的双向转换\n",
    "\n",
    "2. **DataLoader数据加载器**\n",
    "   - 批量数据处理\n",
    "   - 图像预处理流程\n",
    "   - 高效的数据管道设计\n",
    "\n",
    "3. **数据增强技术**\n",
    "   - 提高数据多样性的方法\n",
    "   - 医学影像的特殊考虑\n",
    "   - 防止过拟合的策略\n",
    "\n",
    "### 🧠 关键理解点\n",
    "\n",
    "- **数据预处理**是深度学习成功的基础\n",
    "- **标准化**让模型训练更稳定\n",
    "- **批量处理**大大提高计算效率\n",
    "- **数据增强**提升模型泛化能力\n",
    "\n",
    "### 🚀 下节预告\n",
    "\n",
    "下一节我们将学习R2Gen的核心架构：\n",
    "1. **视觉特征提取器** - CNN如何\"看懂\"医学影像\n",
    "2. **编码器-解码器结构** - 如何从图像生成文本\n",
    "3. **注意力机制** - 让AI学会\"重点关注\"\n",
    "\n",
    "### 💪 练习建议\n",
    "\n",
    "1. 回顾数据处理的每个步骤，理解其必要性\n",
    "2. 思考：为什么要将文字转换成数字？\n",
    "3. 想象一下：如果你是AI，你希望接收什么样格式的数据？\n",
    "\n",
    "---\n",
    "**记住：好的数据预处理是模型成功的一半！** 🎯✨\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
