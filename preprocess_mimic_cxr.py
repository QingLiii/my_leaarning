#!/usr/bin/env python3
"""
MIMIC-CXRæ•°æ®é¢„å¤„ç†è„šæœ¬
åŸºäºR2Gençš„æ•°æ®æ ¼å¼è¦æ±‚ï¼Œå°†MIMIC-CXRåŸå§‹æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""

import os
import json
import pandas as pd
import numpy as np
from collections import defaultdict
import re
from pathlib import Path
import argparse
from tqdm import tqdm

def clean_report_mimic_cxr(report):
    """
    æ¸…ç†MIMIC-CXRæŠ¥å‘Šæ–‡æœ¬
    åŸºäºR2Gençš„æ¸…ç†é€»è¾‘
    """
    if not report or pd.isna(report):
        return ""
    
    report_cleaner = lambda t: t.replace('\n', ' ').replace('__', '_').replace('__', '_').replace('__', '_') \
        .replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('  ', ' ') \
        .replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ') \
        .replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.') \
        .replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '').replace('. 2. ', '. ') \
        .replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ').replace(' 2. ', '. ') \
        .replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \
        .strip().lower().split('. ')
    
    sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', 
                                   t.replace('"', '').replace('/', '').replace('\\\\', '').replace("'", '').strip().lower())
    
    tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != '']
    report = ' . '.join(tokens) + ' .'
    
    return report

def load_mimic_cxr_reports(reports_dir):
    """
    åŠ è½½MIMIC-CXRæŠ¥å‘Šæ•°æ®
    """
    print("ğŸ“„ åŠ è½½MIMIC-CXRæŠ¥å‘Šæ•°æ®...")
    
    reports_data = {}
    reports_path = Path(reports_dir)
    
    # æŸ¥æ‰¾æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶
    report_files = list(reports_path.rglob("*.txt"))
    
    print(f"æ‰¾åˆ° {len(report_files)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
    
    for report_file in tqdm(report_files, desc="åŠ è½½æŠ¥å‘Š"):
        try:
            # ä»æ–‡ä»¶è·¯å¾„æå–study_id
            # è·¯å¾„æ ¼å¼: p10/p10000032/s50414267.txt
            parts = report_file.parts
            if len(parts) >= 3:
                study_id = parts[-1].replace('.txt', '')  # s50414267
                
                with open(report_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£ææŠ¥å‘Šå†…å®¹
                sections = {}
                current_section = None
                current_content = []
                
                for line in content.split('\n'):
                    line = line.strip()
                    if line.endswith(':') and len(line) < 50:  # å¯èƒ½æ˜¯ç« èŠ‚æ ‡é¢˜
                        if current_section:
                            sections[current_section] = '\n'.join(current_content)
                        current_section = line[:-1].upper()
                        current_content = []
                    else:
                        current_content.append(line)
                
                if current_section:
                    sections[current_section] = '\n'.join(current_content)
                
                # æå–FINDINGSå’ŒIMPRESSIONéƒ¨åˆ†
                findings = sections.get('FINDINGS', '')
                impression = sections.get('IMPRESSION', '')
                
                # åˆå¹¶ä¸ºå®Œæ•´æŠ¥å‘Š
                full_report = ''
                if findings:
                    full_report += findings
                if impression:
                    if full_report:
                        full_report += ' '
                    full_report += impression
                
                if full_report:
                    reports_data[study_id] = clean_report_mimic_cxr(full_report)
                    
        except Exception as e:
            print(f"å¤„ç†æŠ¥å‘Šæ–‡ä»¶ {report_file} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(reports_data)} ä¸ªæŠ¥å‘Š")
    return reports_data

def load_mimic_cxr_metadata(metadata_path):
    """
    åŠ è½½MIMIC-CXRå…ƒæ•°æ®
    """
    print("ğŸ“Š åŠ è½½MIMIC-CXRå…ƒæ•°æ®...")
    
    try:
        metadata = pd.read_csv(metadata_path)
        print(f"âœ… å…ƒæ•°æ®åŒ…å« {len(metadata)} æ¡è®°å½•")
        print(f"åˆ—å: {list(metadata.columns)}")
        return metadata
    except Exception as e:
        print(f"âŒ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
        return None

def find_image_files(images_dir):
    """
    æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
    """
    print("ğŸ–¼ï¸ æŸ¥æ‰¾å›¾åƒæ–‡ä»¶...")
    
    image_files = {}
    images_path = Path(images_dir)
    
    # æ”¯æŒçš„å›¾åƒæ ¼å¼
    image_extensions = ['.jpg', '.jpeg', '.png', '.dcm']
    
    for ext in image_extensions:
        files = list(images_path.rglob(f"*{ext}"))
        print(f"æ‰¾åˆ° {len(files)} ä¸ª {ext} æ–‡ä»¶")
        
        for img_file in files:
            # ä»è·¯å¾„æå–study_idå’Œdicom_id
            # è·¯å¾„æ ¼å¼: p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg
            parts = img_file.parts
            if len(parts) >= 4:
                study_id = parts[-2]  # s50414267
                dicom_id = parts[-1].replace(ext, '')  # 02aa804e-bde0afdd-112c0b34-7bc16630-4e384014
                
                if study_id not in image_files:
                    image_files[study_id] = []
                
                # å­˜å‚¨ç›¸å¯¹è·¯å¾„
                rel_path = str(img_file.relative_to(images_path))
                image_files[study_id].append(rel_path)
    
    print(f"âœ… æ‰¾åˆ° {len(image_files)} ä¸ªstudyçš„å›¾åƒæ–‡ä»¶")
    return image_files

def create_train_val_test_split(data, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):
    """
    åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†
    """
    print("ğŸ“Š åˆ›å»ºæ•°æ®é›†åˆ’åˆ†...")
    
    study_ids = list(data.keys())
    np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
    np.random.shuffle(study_ids)
    
    n_total = len(study_ids)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)
    
    train_ids = study_ids[:n_train]
    val_ids = study_ids[n_train:n_train + n_val]
    test_ids = study_ids[n_train + n_val:]
    
    print(f"è®­ç»ƒé›†: {len(train_ids)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_ids)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(test_ids)} æ ·æœ¬")
    
    return {
        'train': train_ids,
        'val': val_ids,
        'test': test_ids
    }

def create_annotation_json(reports_data, image_files, splits, output_path):
    """
    åˆ›å»ºR2Genæ ¼å¼çš„annotation.jsonæ–‡ä»¶
    """
    print("ğŸ“ åˆ›å»ºannotation.jsonæ–‡ä»¶...")
    
    annotation = {
        'train': [],
        'val': [],
        'test': []
    }
    
    for split_name, study_ids in splits.items():
        print(f"å¤„ç† {split_name} é›†...")
        
        for study_id in tqdm(study_ids, desc=f"åˆ›å»º{split_name}é›†"):
            if study_id in reports_data and study_id in image_files:
                report = reports_data[study_id]
                images = image_files[study_id]
                
                # è¿‡æ»¤æ‰ç©ºæŠ¥å‘Š
                if len(report.strip()) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦
                    # å¯¹äºMIMIC-CXRï¼Œé€šå¸¸æ¯ä¸ªstudyåªå–ä¸€å¼ å›¾åƒ
                    # å¦‚æœæœ‰å¤šå¼ å›¾åƒï¼Œå–ç¬¬ä¸€å¼ 
                    image_path = images[0] if images else None
                    
                    if image_path:
                        annotation[split_name].append({
                            'id': study_id,
                            'image_path': [image_path],  # R2Genæ ¼å¼è¦æ±‚åˆ—è¡¨
                            'report': report
                        })
    
    # ç»Ÿè®¡ä¿¡æ¯
    for split_name in ['train', 'val', 'test']:
        print(f"{split_name}é›†: {len(annotation[split_name])} ä¸ªæ ·æœ¬")
    
    # ä¿å­˜annotation.json
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(annotation, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… annotation.jsonå·²ä¿å­˜åˆ°: {output_path}")
    return annotation

def main():
    parser = argparse.ArgumentParser(description='MIMIC-CXRæ•°æ®é¢„å¤„ç†')
    parser.add_argument('--data_dir', type=str, default='datasets/mimic-cxr-dataset',
                        help='MIMIC-CXRæ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='R2Gen-main/data/mimic_cxr',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹MIMIC-CXRæ•°æ®é¢„å¤„ç†...")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 1. åŠ è½½æŠ¥å‘Šæ•°æ®
    reports_dir = os.path.join(args.data_dir, 'mimic-cxr-reports/files')
    if not os.path.exists(reports_dir):
        print(f"âŒ æŠ¥å‘Šç›®å½•ä¸å­˜åœ¨: {reports_dir}")
        return
    
    reports_data = load_mimic_cxr_reports(reports_dir)
    
    if not reports_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æŠ¥å‘Šæ•°æ®")
        return
    
    # 2. æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
    images_dir = os.path.join(args.data_dir, 'official_data_iccv_final/files')
    if not os.path.exists(images_dir):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {images_dir}")
        return
    
    image_files = find_image_files(images_dir)
    
    if not image_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶")
        return
    
    # 3. åŒ¹é…æŠ¥å‘Šå’Œå›¾åƒ
    matched_data = {}
    for study_id in reports_data:
        if study_id in image_files:
            matched_data[study_id] = {
                'report': reports_data[study_id],
                'images': image_files[study_id]
            }
    
    print(f"âœ… åŒ¹é…åˆ° {len(matched_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if args.max_samples and len(matched_data) > args.max_samples:
        study_ids = list(matched_data.keys())[:args.max_samples]
        matched_data = {sid: matched_data[sid] for sid in study_ids}
        print(f"ğŸ”„ é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {len(matched_data)}")
    
    # 4. åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
    splits = create_train_val_test_split(matched_data)
    
    # 5. åˆ›å»ºannotation.json
    annotation_path = os.path.join(args.output_dir, 'annotation.json')
    annotation = create_annotation_json(reports_data, image_files, splits, annotation_path)
    
    # 6. åˆ›å»ºå›¾åƒè½¯é“¾æ¥
    images_output_dir = os.path.join(args.output_dir, 'images')
    os.makedirs(images_output_dir, exist_ok=True)
    
    print("ğŸ”— åˆ›å»ºå›¾åƒè½¯é“¾æ¥...")
    images_source_dir = os.path.abspath(images_dir)
    images_target_dir = os.path.abspath(images_output_dir)
    
    # åˆ›å»ºè½¯é“¾æ¥æŒ‡å‘åŸå§‹å›¾åƒç›®å½•
    link_path = os.path.join(images_target_dir, 'files')
    if not os.path.exists(link_path):
        try:
            os.symlink(images_source_dir, link_path)
            print(f"âœ… åˆ›å»ºè½¯é“¾æ¥: {link_path} -> {images_source_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºè½¯é“¾æ¥å¤±è´¥: {e}")
            print("å°†å¤åˆ¶å›¾åƒè·¯å¾„ä¿¡æ¯åˆ°é…ç½®ä¸­")
    
    print("ğŸ‰ MIMIC-CXRæ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“„ annotation.json: {annotation_path}")
    print(f"ğŸ–¼ï¸ å›¾åƒç›®å½•: {images_output_dir}")

if __name__ == "__main__":
    main()
