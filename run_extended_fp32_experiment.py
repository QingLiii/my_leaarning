#!/usr/bin/env python3
"""
æ‰©å±•çš„FP32å®éªŒ - åŸºäºæˆåŠŸçš„5epoché…ç½®è¿è¡Œ15epoch
"""

import os
import sys
import time
import json
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('R2Gen-main')

from modules.tokenizers import Tokenizer
from modules.dataloaders import R2DataLoader
from modules.metrics import compute_scores
from modules.optimizers import build_optimizer, build_lr_scheduler
from modules.loss import compute_loss
from models.r2gen import R2GenModel
from modules.wandb_logger import WandBLogger
from modules.memory_monitor import MemoryMonitor
from modules.enhanced_trainer import EnhancedTrainer

class ExtendedExperimentConfig:
    """æ‰©å±•å®éªŒé…ç½® - åŸºäºæˆåŠŸçš„é…ç½®"""
    
    def __init__(self, epochs=15):
        # åŸºç¡€é…ç½®
        self.epochs = epochs
        
        # æ•°æ®é›†ç›¸å…³
        self.image_dir = 'datasets/iu_xray/images/'
        self.ann_path = 'datasets/iu_xray/annotation.json'
        self.dataset_name = 'iu_xray'
        self.max_seq_length = 60
        self.threshold = 3
        self.num_workers = 2
        self.batch_size = 12
        
        # è®­ç»ƒç›¸å…³
        self.seed = 9223
        self.n_gpu = 1
        self.save_period = 1
        self.monitor_mode = 'max'
        self.monitor_metric = 'BLEU_4'
        self.early_stop = 50
        self.resume = None
        self.validate_every = 1  # æ¯ä¸ªepochéªŒè¯
        
        # ä¼˜åŒ–å™¨ç›¸å…³ - æŒ‰è®ºæ–‡é…ç½®
        self.optim = 'Adam'
        self.lr_ve = 5e-5      # visual extractorå­¦ä¹ ç‡
        self.lr_ed = 1e-4      # encoder-decoderå­¦ä¹ ç‡
        self.weight_decay = 0
        self.amsgrad = True
        
        # å­¦ä¹ ç‡è°ƒåº¦ - æŒ‰è®ºæ–‡è¦æ±‚æ¯epochè¡°å‡0.8
        self.lr_scheduler = 'StepLR'
        self.step_size = 1     # æ¯ä¸ªepochè¡°å‡
        self.gamma = 0.8       # è¡°å‡å› å­0.8
        
        # æ¨¡å‹ç›¸å…³
        self.d_model = 512
        self.d_ff = 512
        self.d_vf = 2048
        self.num_heads = 8
        self.num_layers = 3
        self.dropout = 0.1
        self.logit_layers = 1
        self.bos_idx = 0
        self.eos_idx = 0
        self.pad_idx = 0
        self.use_bn = True
        self.drop_prob_lm = 0.5
        
        # è®°å¿†æ¨¡å—
        self.rm_num_slots = 3
        self.rm_num_heads = 8
        self.rm_d_model = 512
        
        # è§†è§‰ç‰¹å¾æå–
        self.visual_extractor = 'resnet101'
        self.visual_extractor_pretrained = True
        
        # é‡‡æ ·ç›¸å…³
        self.sample_method = 'beam_search'
        self.beam_size = 3
        self.temperature = 1.0
        self.sample_n = 1
        
        # æ··åˆç²¾åº¦é…ç½®
        self.mixed_precision = False
        self.precision_mode = 'fp32'
        
        # ä¿å­˜è·¯å¾„
        self.save_dir = f'results/extended_experiment/fp32'
        self.record_dir = f'results/extended_experiment/fp32'
        self.experiment_name = f'R2Gen_fp32_extended'

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰©å±•FP32å®éªŒ')
    parser.add_argument('--epochs', type=int, default=15, help='è®­ç»ƒepochæ•°')
    
    args = parser.parse_args()
    
    print("ğŸš€ R2Genæ‰©å±•FP32å®éªŒ")
    print(f"   è®­ç»ƒepochs: {args.epochs}")
    print(f"   å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # éªŒè¯å‰ç½®æ¡ä»¶
    print(f"\nğŸ“‹ éªŒè¯å‰ç½®æ¡ä»¶...")
    
    # éªŒè¯tokenizerè§£ç 
    test_args = ExtendedExperimentConfig()
    tokenizer = Tokenizer(test_args)
    test_text = 'the lungs are clear'
    encoded = tokenizer(test_text)
    decoded = tokenizer.decode(encoded)
    
    if len(decoded) == 0:
        print("âŒ Tokenizerè§£ç å¤±è´¥ï¼Œå®éªŒç»ˆæ­¢")
        return
    else:
        print("âœ… Tokenizerè§£ç æ­£å¸¸")
    
    # åˆ›å»ºé…ç½®
    config = ExtendedExperimentConfig(args.epochs)
    
    print(f"\nğŸš€ å¼€å§‹FP32æ‰©å±•è®­ç»ƒ...")
    print(f"   Batch Size: {config.batch_size}")
    print(f"   Epochs: {args.epochs}")
    print(f"   å­¦ä¹ ç‡è°ƒåº¦: æ¯epochè¡°å‡{config.gamma}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print(f"   ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_dataloader = R2DataLoader(config, tokenizer, split='train', shuffle=True)
    val_dataloader = R2DataLoader(config, tokenizer, split='val', shuffle=False)
    test_dataloader = R2DataLoader(config, tokenizer, split='test', shuffle=False)
    
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataloader.dataset)}")
    print(f"   éªŒè¯æ ·æœ¬: {len(val_dataloader.dataset)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_dataloader.dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"   ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    model = R2GenModel(config, tokenizer).cuda()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = build_optimizer(config, model)
    lr_scheduler = build_lr_scheduler(config, optimizer)
    
    # åˆå§‹åŒ–WandB
    print(f"   ğŸ“ˆ åˆå§‹åŒ–WandBç›‘æ§...")
    import os
    os.environ['WANDB_API_KEY'] = '68c9ce2a167992d06678c4fdc0d1075b5dfff922'
    wandb_logger = WandBLogger(project_name="R2Gen-Extended-Experiment")
    
    wandb_config = {
        'precision': 'fp32',
        'mixed_precision': False,
        'batch_size': config.batch_size,
        'epochs': args.epochs,
        'learning_rate_ve': config.lr_ve,
        'learning_rate_ed': config.lr_ed,
        'lr_scheduler': config.lr_scheduler,
        'lr_decay_factor': config.gamma,
        'lr_decay_step': config.step_size,
        'model': 'R2Gen',
        'dataset': config.dataset_name,
        'experiment_type': 'extended_fp32',
        'optimizer': config.optim,
        'rm_num_slots': config.rm_num_slots,
        'rm_d_model': config.rm_d_model,
        'beam_size': config.beam_size,
        'd_model': config.d_model,
        'num_layers': config.num_layers,
        'num_heads': config.num_heads
    }
    
    run_name = f"R2Gen_fp32_extended_bs{config.batch_size}_ep{args.epochs}"
    wandb_logger.init_run(wandb_config, run_name=run_name)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print(f"   ğŸ¯ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = EnhancedTrainer(
        model=model,
        criterion=compute_loss,
        metric_ftns=compute_scores,
        optimizer=optimizer,
        args=config,
        lr_scheduler=lr_scheduler,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        test_dataloader=test_dataloader,
        wandb_logger=wandb_logger,
        enable_wandb=True
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"   ğŸƒ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    try:
        trainer.train()
        training_time = (time.time() - start_time) / 3600  # è½¬æ¢ä¸ºå°æ—¶
        
        print(f"   âœ… æ‰©å±•FP32è®­ç»ƒæˆåŠŸå®Œæˆ!")
        wandb_logger.finish()
        
        # ä¿å­˜ç»“æœ
        result = {
            'precision': 'fp32',
            'status': 'success',
            'training_time_hours': training_time,
            'batch_size': config.batch_size,
            'epochs': args.epochs,
            'model_path': f'{config.save_dir}/model_best.pth',
            'lr_decay_factor': config.gamma,
            'lr_decay_step': config.step_size
        }
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'extended_fp32_results_{timestamp}.json'
        
        with open(results_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"\nğŸ‰ æ‰©å±•FP32å®éªŒå®Œæˆ!")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} å°æ—¶")
        print(f"   ğŸ“ æ¨¡å‹ä¿å­˜: {result['model_path']}")
        print(f"   ğŸ“Š ç»“æœæ–‡ä»¶: {results_file}")
        
    except Exception as e:
        print(f"   âŒ æ‰©å±•FP32è®­ç»ƒå¤±è´¥: {str(e)}")
        wandb_logger.finish()
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        result = {
            'precision': 'fp32',
            'status': 'failed',
            'error': str(e),
            'batch_size': config.batch_size,
            'epochs': args.epochs
        }
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'extended_fp32_results_{timestamp}.json'
        
        with open(results_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"   ğŸ“Š é”™è¯¯è®°å½•: {results_file}")

if __name__ == "__main__":
    main()
