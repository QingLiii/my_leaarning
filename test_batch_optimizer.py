#!/usr/bin/env python3
"""
æµ‹è¯•Batch Sizeä¼˜åŒ–å™¨
"""

import sys
import torch
import time

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append('R2Gen-main')
sys.path.append('R2Gen-main/modules')

from R2Gen_main.modules.memory_monitor import MemoryMonitor
from R2Gen_main.modules.batch_size_optimizer import BatchSizeOptimizer


class SimpleTestModel(torch.nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(3, 64, 3, padding=1)
        self.pool = torch.nn.AdaptiveAvgPool2d((7, 7))
        self.fc = torch.nn.Linear(64 * 7 * 7, 512)
        self.output = torch.nn.Linear(512, 1000)
    
    def forward(self, images, reports_ids=None, mode='train'):
        x = self.conv(images)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        x = self.output(x)
        return x


def test_batch_optimizer():
    """æµ‹è¯•Batch Sizeä¼˜åŒ–å™¨"""
    print("ğŸ§ª æµ‹è¯•Batch Sizeä¼˜åŒ–å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œç›‘æ§å™¨
    model = SimpleTestModel().cuda()
    memory_monitor = MemoryMonitor(target_utilization=0.85)
    
    print(f"ğŸ“Š åˆå§‹æ˜¾å­˜çŠ¶æ€:")
    memory_monitor.print_status()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = BatchSizeOptimizer(model, memory_monitor)
    
    # è¿è¡Œä¼˜åŒ–
    print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–...")
    optimal_size, results = optimizer.find_optimal_batch_size(
        start_size=2,
        max_size=16,
        precision='fp32',
        step_size=2
    )
    
    # æ‰“å°æŠ¥å‘Š
    optimizer.print_optimization_report()
    
    print(f"\nğŸ“Š æœ€ç»ˆæ˜¾å­˜çŠ¶æ€:")
    memory_monitor.print_status()
    
    return optimal_size, results


if __name__ == "__main__":
    try:
        optimal_size, results = test_batch_optimizer()
        print(f"\nâœ… æµ‹è¯•å®Œæˆ! æœ€ä¼˜batch size: {optimal_size}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
