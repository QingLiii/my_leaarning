{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2Genå­¦ä¹ ç³»åˆ— - ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸Tokenization\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "1. R2Gené¡¹ç›®çš„èƒŒæ™¯å’Œæ ¸å¿ƒæ€æƒ³\n",
    "2. åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„æŒ‘æˆ˜\n",
    "3. æ–‡æœ¬tokenizationçš„åŸç†å’Œå®ç°\n",
    "4. è¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹\n",
    "5. æ•°æ®é¢„å¤„ç†çš„é‡è¦æ€§\n",
    "\n",
    "## ğŸ“š é¡¹ç›®èƒŒæ™¯\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯R2Genï¼Ÿ\n",
    "R2Gen (Radiology Report Generation) æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆç³»ç»Ÿã€‚å®ƒçš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä½¿ç”¨äº†**Memory-driven Transformer**æ¶æ„ï¼Œèƒ½å¤Ÿï¼š\n",
    "\n",
    "- ğŸ“¸ **åˆ†æåŒ»å­¦å½±åƒ**ï¼šå¤„ç†Xå…‰ç‰‡ã€CTæ‰«æç­‰åŒ»å­¦å›¾åƒ\n",
    "- ğŸ§  **åˆ©ç”¨åŒ»å­¦çŸ¥è¯†**ï¼šé€šè¿‡å…³ç³»è®°å¿†æ¨¡å—å­˜å‚¨å’Œåˆ©ç”¨å…ˆéªŒåŒ»å­¦çŸ¥è¯†\n",
    "- ğŸ“ **ç”ŸæˆæŠ¥å‘Š**ï¼šè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–çš„æ”¾å°„å­¦æŠ¥å‘Š\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ\n",
    "1. **æé«˜æ•ˆç‡**ï¼šå‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…\n",
    "2. **æ ‡å‡†åŒ–**ï¼šç¡®ä¿æŠ¥å‘Šæ ¼å¼å’Œæœ¯è¯­çš„ä¸€è‡´æ€§\n",
    "3. **è¾…åŠ©è¯Šæ–­**ï¼šä¸ºåŒ»ç”Ÿæä¾›åˆæ­¥çš„åˆ†æç»“æœ\n",
    "\n",
    "### æŠ€æœ¯æŒ‘æˆ˜\n",
    "- åŒ»å­¦æœ¯è¯­çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§è¦æ±‚\n",
    "- å›¾åƒç‰¹å¾ä¸æ–‡æœ¬æè¿°çš„å¯¹åº”å…³ç³»\n",
    "- é•¿åºåˆ—æ–‡æœ¬ç”Ÿæˆçš„è¿è´¯æ€§\n",
    "- åŒ»å­¦çŸ¥è¯†çš„æœ‰æ•ˆåˆ©ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾\n",
    "\n",
    "æˆ‘ä»¬å°†é€šè¿‡ä»¥ä¸‹æ­¥éª¤é€æ­¥å­¦ä¹ R2Genï¼š\n",
    "\n",
    "```\n",
    "ğŸ“– ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸Tokenization (å½“å‰)\n",
    "   â”œâ”€â”€ æ–‡æœ¬é¢„å¤„ç†\n",
    "   â”œâ”€â”€ TokenizationåŸç†\n",
    "   â””â”€â”€ è¯æ±‡è¡¨æ„å»º\n",
    "\n",
    "ğŸ–¼ï¸ ç¬¬2éƒ¨åˆ†ï¼šè§†è§‰ç‰¹å¾æå–\n",
    "   â”œâ”€â”€ CNNç‰¹å¾æå–å™¨\n",
    "   â”œâ”€â”€ å›¾åƒé¢„å¤„ç†\n",
    "   â””â”€â”€ ç‰¹å¾å¯è§†åŒ–\n",
    "\n",
    "ğŸ”§ ç¬¬3éƒ¨åˆ†ï¼šTransformeråŸºç¡€ç»„ä»¶\n",
    "   â”œâ”€â”€ æ³¨æ„åŠ›æœºåˆ¶\n",
    "   â”œâ”€â”€ ç¼–ç å™¨-è§£ç å™¨\n",
    "   â””â”€â”€ ä½ç½®ç¼–ç \n",
    "\n",
    "ğŸ§  ç¬¬4éƒ¨åˆ†ï¼šå…³ç³»è®°å¿†æ¨¡å—\n",
    "   â”œâ”€â”€ è®°å¿†æœºåˆ¶åŸç†\n",
    "   â”œâ”€â”€ æ¡ä»¶å±‚å½’ä¸€åŒ–\n",
    "   â””â”€â”€ çŸ¥è¯†å­˜å‚¨ä¸æ£€ç´¢\n",
    "\n",
    "ğŸš€ ç¬¬5éƒ¨åˆ†ï¼šå®Œæ•´æ¨¡å‹è®­ç»ƒ\n",
    "   â”œâ”€â”€ æŸå¤±å‡½æ•°\n",
    "   â”œâ”€â”€ è®­ç»ƒå¾ªç¯\n",
    "   â””â”€â”€ æ¨¡å‹è¯„ä¼°\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "R2Genæ”¯æŒä¸¤ä¸ªä¸»è¦çš„åŒ»å­¦æ•°æ®é›†ï¼š\n",
    "\n",
    "### 1. IU X-Rayæ•°æ®é›†\n",
    "- **å›¾åƒç±»å‹**ï¼šèƒ¸éƒ¨Xå…‰ç‰‡\n",
    "- **ç‰¹ç‚¹**ï¼šæ¯ä¸ªç—…ä¾‹åŒ…å«ä¸¤å¼ å›¾åƒï¼ˆæ­£é¢å’Œä¾§é¢ï¼‰\n",
    "- **æŠ¥å‘Š**ï¼šç»“æ„åŒ–çš„æ”¾å°„å­¦æŠ¥å‘Š\n",
    "\n",
    "### 2. MIMIC-CXRæ•°æ®é›†\n",
    "- **å›¾åƒç±»å‹**ï¼šèƒ¸éƒ¨Xå…‰ç‰‡\n",
    "- **ç‰¹ç‚¹**ï¼šå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå•å¼ å›¾åƒ\n",
    "- **æŠ¥å‘Š**ï¼šè¯¦ç»†çš„ä¸´åºŠæŠ¥å‘Š\n",
    "\n",
    "### æ•°æ®æ ¼å¼ç¤ºä¾‹\n",
    "```json\n",
    "{\n",
    "  \"id\": \"patient_001\",\n",
    "  \"image_path\": [\"frontal.jpg\", \"lateral.jpg\"],\n",
    "  \"report\": \"The lungs are clear. No acute cardiopulmonary abnormality.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# å¯¼å…¥é«˜åˆ†è¾¨ç‡é…ç½®\n",
    "try:\n",
    "    from matplotlib_config import setup_high_quality_plots, save_high_quality_figure, create_publication_figure, get_figure_size\n",
    "    setup_high_quality_plots()\n",
    "    print(\"âœ… å·²å¯ç”¨ç§‘ç ”çº§åˆ«é«˜åˆ†è¾¨ç‡ç»˜å›¾é…ç½®\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ matplotlib_config.pyæœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é«˜åˆ†è¾¨ç‡é…ç½®\")\n",
    "    # æ‰‹åŠ¨è®¾ç½®é«˜åˆ†è¾¨ç‡\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    WORDCLOUD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WORDCLOUD_AVAILABLE = False\n",
    "    print(\"âš ï¸ WordCloudåº“æœªå®‰è£…ï¼Œè¯äº‘åŠŸèƒ½å°†è¢«è·³è¿‡\")\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆï¼\")\n",
    "print(\"ğŸ“š å‡†å¤‡å¼€å§‹å­¦ä¹ R2Gençš„æ•°æ®å¤„ç†æ¨¡å—...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¤ æ–‡æœ¬Tokenizationè¯¦è§£\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯Tokenizationï¼Ÿ\n",
    "Tokenizationæ˜¯å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—çš„è¿‡ç¨‹ã€‚åœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä¸­ï¼Œè¿™ä¸ªæ­¥éª¤å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºï¼š\n",
    "\n",
    "1. **åŒ»å­¦æœ¯è¯­å¤„ç†**ï¼šæ­£ç¡®å¤„ç†ä¸“ä¸šåŒ»å­¦è¯æ±‡\n",
    "2. **æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€ä¸åŒæ ¼å¼çš„æŠ¥å‘Š\n",
    "3. **æ•°å€¼åŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯å¤„ç†çš„æ•°å­—\n",
    "\n",
    "### R2Gençš„Tokenizationæµç¨‹\n",
    "```\n",
    "åŸå§‹æŠ¥å‘Š â†’ æ–‡æœ¬æ¸…ç† â†’ åˆ†è¯ â†’ è¯æ±‡è¡¨æ˜ å°„ â†’ æ•°å­—åºåˆ—\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®©æˆ‘ä»¬ä»R2Genæºç ä¸­æå–å¹¶ç®€åŒ–Tokenizerç±»\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"ç®€åŒ–ç‰ˆçš„R2Gen Tokenizerï¼Œç”¨äºå­¦ä¹ ç›®çš„\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=3):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–tokenizer\n",
    "        \n",
    "        Args:\n",
    "            threshold: è¯æ±‡å‡ºç°çš„æœ€å°é¢‘æ¬¡ï¼Œä½äºæ­¤é¢‘æ¬¡çš„è¯ä¼šè¢«æ ‡è®°ä¸º<unk>\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.token2idx = {}  # è¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        self.idx2token = {}  # ç´¢å¼•åˆ°è¯æ±‡çš„æ˜ å°„\n",
    "        \n",
    "    def clean_report_iu_xray(self, report):\n",
    "        \"\"\"\n",
    "        æ¸…ç†IU X-Rayæ•°æ®é›†çš„æŠ¥å‘Šæ–‡æœ¬\n",
    "        è¿™ä¸ªå‡½æ•°å±•ç¤ºäº†åŒ»å­¦æ–‡æœ¬é¢„å¤„ç†çš„å¤æ‚æ€§\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å¥å­åˆ†éš”ç¬¦å’Œç¼–å·\n",
    "        report_cleaner = lambda t: t.replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '') \\\n",
    "            .replace('. 2. ', '. ').replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ') \\\n",
    "            .replace(' 2. ', '. ').replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "            .strip().lower().split('. ')\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šæ¸…ç†æ¯ä¸ªå¥å­ä¸­çš„æ ‡ç‚¹ç¬¦å·\n",
    "        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\\\[\\\\]{}]', '', \n",
    "                                       t.replace('\"', '').replace('/', '').replace('\\\\\\\\', '').replace(\"'\", '').strip().lower())\n",
    "        \n",
    "        # åº”ç”¨æ¸…ç†å‡½æ•°\n",
    "        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != '']\n",
    "        report = ' . '.join(tokens) + ' .'\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def create_vocabulary(self, reports):\n",
    "        \"\"\"\n",
    "        ä»æŠ¥å‘Šåˆ—è¡¨ä¸­åˆ›å»ºè¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            reports: æŠ¥å‘Šæ–‡æœ¬åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        print(\"ğŸ” å¼€å§‹åˆ†ææŠ¥å‘Šï¼Œæ„å»ºè¯æ±‡è¡¨...\")\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰è¯æ±‡\n",
    "        total_tokens = []\n",
    "        for report in reports:\n",
    "            cleaned_report = self.clean_report_iu_xray(report)\n",
    "            tokens = cleaned_report.split()\n",
    "            total_tokens.extend(tokens)\n",
    "        \n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        counter = Counter(total_tokens)\n",
    "        print(f\"ğŸ“Š æ€»å…±å‘ç° {len(counter)} ä¸ªä¸åŒçš„è¯æ±‡\")\n",
    "        print(f\"ğŸ“Š æ€»è¯æ±‡æ•°é‡: {len(total_tokens)}\")\n",
    "        \n",
    "        # è¿‡æ»¤ä½é¢‘è¯æ±‡\n",
    "        vocab = [k for k, v in counter.items() if v >= self.threshold] + ['<unk>']\n",
    "        vocab.sort()\n",
    "        \n",
    "        print(f\"ğŸ“Š è¿‡æ»¤åè¯æ±‡è¡¨å¤§å°: {len(vocab)} (é˜ˆå€¼: {self.threshold})\")\n",
    "        \n",
    "        # åˆ›å»ºæ˜ å°„å­—å…¸\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        \n",
    "        for idx, token in enumerate(vocab):\n",
    "            self.token2idx[token] = idx + 1  # ä»1å¼€å§‹ï¼Œ0ç•™ç»™padding\n",
    "            self.idx2token[idx + 1] = token\n",
    "        \n",
    "        return counter\n",
    "    \n",
    "    def tokenize(self, report):\n",
    "        \"\"\"\n",
    "        å°†æŠ¥å‘Šè½¬æ¢ä¸ºtoken IDåºåˆ—\n",
    "        \n",
    "        Args:\n",
    "            report: åŸå§‹æŠ¥å‘Šæ–‡æœ¬\n",
    "            \n",
    "        Returns:\n",
    "            token IDåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        cleaned_report = self.clean_report_iu_xray(report)\n",
    "        tokens = cleaned_report.split()\n",
    "        \n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token2idx:\n",
    "                ids.append(self.token2idx[token])\n",
    "            else:\n",
    "                ids.append(self.token2idx['<unk>'])  # æœªçŸ¥è¯æ±‡\n",
    "        \n",
    "        # æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°\n",
    "        ids = [0] + ids + [0]  # 0ä½œä¸ºç‰¹æ®Šæ ‡è®°\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        å°†token IDåºåˆ—è½¬æ¢å›æ–‡æœ¬\n",
    "        \n",
    "        Args:\n",
    "            ids: token IDåˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        txt = ''\n",
    "        for i, idx in enumerate(ids):\n",
    "            if idx > 0:  # è·³è¿‡ç‰¹æ®Šæ ‡è®°\n",
    "                if i >= 1:\n",
    "                    txt += ' '\n",
    "                txt += self.idx2token[idx]\n",
    "            else:\n",
    "                break\n",
    "        return txt\n",
    "\n",
    "print(\"âœ… SimpleTokenizerç±»å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å®è·µï¼šä½¿ç”¨Tokenizerå¤„ç†åŒ»å­¦æŠ¥å‘Š\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬ç”¨ä¸€äº›ç¤ºä¾‹åŒ»å­¦æŠ¥å‘Šæ¥æµ‹è¯•æˆ‘ä»¬çš„tokenizerï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€äº›ç¤ºä¾‹åŒ»å­¦æŠ¥å‘Š\n",
    "sample_reports = [\n",
    "    \"The lungs are clear. No acute cardiopulmonary abnormality. Heart size is normal.\",\n",
    "    \"Bilateral lower lobe pneumonia. Cardiomegaly is present. Pleural effusion noted.\",\n",
    "    \"No acute findings. Lungs are clear bilaterally. Normal heart size and contour.\",\n",
    "    \"Pneumothorax on the right side. Lung collapse observed. Immediate attention required.\",\n",
    "    \"Chronic obstructive pulmonary disease. Hyperinflation of lungs. No acute changes.\",\n",
    "    \"Normal chest X-ray. No abnormalities detected. Heart and lungs appear normal.\",\n",
    "    \"Pulmonary edema present. Enlarged heart shadow. Bilateral infiltrates seen.\",\n",
    "    \"Rib fractures on left side. No pneumothorax. Lungs are otherwise clear.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ ç¤ºä¾‹åŒ»å­¦æŠ¥å‘Šå‡†å¤‡å®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š æ€»å…± {len(sample_reports)} ä»½æŠ¥å‘Š\")\n",
    "print(\"\\nğŸ“ æŠ¥å‘Šç¤ºä¾‹:\")\n",
    "for i, report in enumerate(sample_reports[:3], 1):\n",
    "    print(f\"{i}. {report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºtokenizerå®ä¾‹å¹¶æ„å»ºè¯æ±‡è¡¨\n",
    "tokenizer = SimpleTokenizer(threshold=2)  # è®¾ç½®è¾ƒä½çš„é˜ˆå€¼ä»¥ä¾¿è§‚å¯Ÿæ›´å¤šè¯æ±‡\n",
    "\n",
    "# æ„å»ºè¯æ±‡è¡¨\n",
    "word_counter = tokenizer.create_vocabulary(sample_reports)\n",
    "\n",
    "print(f\"\\nğŸ“š è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(tokenizer.token2idx)}\")\n",
    "print(f\"ğŸ“Š æœ€é«˜é¢‘è¯æ±‡: {word_counter.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¯é¢‘åˆ†å¸ƒ\n",
    "# ä½¿ç”¨ç§‘ç ”çº§åˆ«çš„å›¾ç‰‡å°ºå¯¸\n",
    "try:\n",
    "    fig = create_publication_figure(figsize=get_figure_size('presentation'))\n",
    "except NameError:\n",
    "    fig = plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "# å­å›¾1ï¼šè¯é¢‘åˆ†å¸ƒ\n",
    "plt.subplot(2, 2, 1)\n",
    "top_words = word_counter.most_common(15)\n",
    "words, counts = zip(*top_words)\n",
    "plt.barh(range(len(words)), counts)\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('å‡ºç°é¢‘æ¬¡')\n",
    "plt.title('Top 15 é«˜é¢‘è¯æ±‡')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# å­å›¾2ï¼šè¯æ±‡é•¿åº¦åˆ†å¸ƒ\n",
    "plt.subplot(2, 2, 2)\n",
    "word_lengths = [len(word) for word in word_counter.keys()]\n",
    "plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('è¯æ±‡é•¿åº¦')\n",
    "plt.ylabel('è¯æ±‡æ•°é‡')\n",
    "plt.title('è¯æ±‡é•¿åº¦åˆ†å¸ƒ')\n",
    "\n",
    "# å­å›¾3ï¼šè¯é¢‘åˆ†å¸ƒç›´æ–¹å›¾\n",
    "plt.subplot(2, 2, 3)\n",
    "freq_counts = list(word_counter.values())\n",
    "plt.hist(freq_counts, bins=range(1, max(freq_counts)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('å‡ºç°é¢‘æ¬¡')\n",
    "plt.ylabel('è¯æ±‡æ•°é‡')\n",
    "plt.title('è¯é¢‘åˆ†å¸ƒç›´æ–¹å›¾')\n",
    "plt.yscale('log')\n",
    "\n",
    "# å­å›¾4ï¼šè¯äº‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "plt.subplot(2, 2, 4)\n",
    "if WORDCLOUD_AVAILABLE:\n",
    "    # æé«˜è¯äº‘åˆ†è¾¨ç‡\n",
    "    wordcloud = WordCloud(width=800, height=600, background_color='white', max_words=100).generate_from_frequencies(word_counter)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('åŒ»å­¦è¯æ±‡è¯äº‘')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'WordCloud\\\\nåº“æœªå®‰è£…', ha='center', va='center', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.title('è¯äº‘åŠŸèƒ½ä¸å¯ç”¨')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡\n",
    "try:\n",
    "    save_high_quality_figure('vocabulary_analysis_high_res.png')\n",
    "except NameError:\n",
    "    plt.savefig('vocabulary_analysis_high_res.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(\"ğŸ’¾ é«˜åˆ†è¾¨ç‡å›¾ç‰‡å·²ä¿å­˜: vocabulary_analysis_high_res.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š è¯æ±‡åˆ†æå¯è§†åŒ–å®Œæˆï¼\")\n",
    "print(\"ğŸ¯ å›¾ç‰‡å·²ä¿å­˜ä¸ºç§‘ç ”çº§åˆ«é«˜åˆ†è¾¨ç‡æ ¼å¼ (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Tokenizationè¿‡ç¨‹æ¼”ç¤º\n",
    "\n",
    "è®©æˆ‘ä»¬è¯¦ç»†è§‚å¯Ÿä¸€ä¸ªæŠ¥å‘Šæ˜¯å¦‚ä½•è¢«tokenizeçš„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹æŠ¥å‘Šè¿›è¡Œè¯¦ç»†åˆ†æ\n",
    "example_report = \"Bilateral lower lobe pneumonia. Cardiomegaly is present. Pleural effusion noted.\"\n",
    "\n",
    "print(\"ğŸ” Tokenizationè¿‡ç¨‹æ¼”ç¤º\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ğŸ“ åŸå§‹æŠ¥å‘Š: {example_report}\")\n",
    "print()\n",
    "\n",
    "# æ­¥éª¤1ï¼šæ–‡æœ¬æ¸…ç†\n",
    "cleaned = tokenizer.clean_report_iu_xray(example_report)\n",
    "print(f\"ğŸ§¹ æ¸…ç†å: {cleaned}\")\n",
    "print()\n",
    "\n",
    "# æ­¥éª¤2ï¼šåˆ†è¯\n",
    "tokens = cleaned.split()\n",
    "print(f\"ğŸ”¤ åˆ†è¯ç»“æœ: {tokens}\")\n",
    "print(f\"ğŸ“Š è¯æ±‡æ•°é‡: {len(tokens)}\")\n",
    "print()\n",
    "\n",
    "# æ­¥éª¤3ï¼šè½¬æ¢ä¸ºID\n",
    "token_ids = tokenizer.tokenize(example_report)\n",
    "print(f\"ğŸ”¢ Token IDs: {token_ids}\")\n",
    "print()\n",
    "\n",
    "# æ­¥éª¤4ï¼šè§£ç éªŒè¯\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"ğŸ”„ è§£ç ç»“æœ: {decoded}\")\n",
    "print()\n",
    "\n",
    "# è¯¦ç»†çš„è¯æ±‡æ˜ å°„\n",
    "print(\"ğŸ“‹ è¯¦ç»†çš„è¯æ±‡æ˜ å°„:\")\n",
    "print(\"-\" * 30)\n",
    "for i, token in enumerate(tokens):\n",
    "    token_id = tokenizer.token2idx.get(token, tokenizer.token2idx['<unk>'])\n",
    "    print(f\"{i+1:2d}. '{token}' â†’ {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æœ¬èŠ‚æ€»ç»“\n",
    "\n",
    "### ğŸ¯ æˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "1. **R2Gené¡¹ç›®èƒŒæ™¯**\n",
    "   - åŒ»å­¦æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆçš„é‡è¦æ€§\n",
    "   - Memory-driven Transformerçš„æ ¸å¿ƒæ€æƒ³\n",
    "   - æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "2. **æ–‡æœ¬é¢„å¤„ç†çš„é‡è¦æ€§**\n",
    "   - åŒ»å­¦æ–‡æœ¬çš„ç‰¹æ®Šæ€§\n",
    "   - æ ‡å‡†åŒ–å¤„ç†çš„å¿…è¦æ€§\n",
    "   - æ¸…ç†è§„åˆ™çš„è®¾è®¡åŸåˆ™\n",
    "\n",
    "3. **Tokenizationæ ¸å¿ƒæ¦‚å¿µ**\n",
    "   - è¯æ±‡è¡¨æ„å»ºç­–ç•¥\n",
    "   - é¢‘æ¬¡é˜ˆå€¼çš„ä½œç”¨\n",
    "   - æœªçŸ¥è¯æ±‡çš„å¤„ç†\n",
    "\n",
    "4. **å®è·µæŠ€èƒ½**\n",
    "   - å®ç°ç®€åŒ–ç‰ˆtokenizer\n",
    "   - è¯é¢‘åˆ†æå’Œå¯è§†åŒ–\n",
    "   - æ€§èƒ½è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "### ğŸ”‘ å…³é”®è¦ç‚¹\n",
    "\n",
    "- **æ•°æ®è´¨é‡å†³å®šæ¨¡å‹æ€§èƒ½**ï¼šå¥½çš„é¢„å¤„ç†æ˜¯æˆåŠŸçš„ä¸€åŠ\n",
    "- **é¢†åŸŸç‰¹å¼‚æ€§**ï¼šåŒ»å­¦æ–‡æœ¬éœ€è¦ä¸“é—¨çš„å¤„ç†ç­–ç•¥\n",
    "- **å¹³è¡¡æ€§è€ƒè™‘**ï¼šè¯æ±‡è¡¨å¤§å°ä¸è¦†ç›–ç‡çš„æƒè¡¡\n",
    "- **å¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªå¤„ç†æ­¥éª¤éƒ½åº”è¯¥æ˜¯å¯ç†è§£å’Œå¯éªŒè¯çš„\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ è®¡åˆ’\n",
    "\n",
    "åœ¨ä¸‹ä¸€ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "\n",
    "ğŸ“¸ **è§†è§‰ç‰¹å¾æå–**\n",
    "- CNNç‰¹å¾æå–å™¨çš„å·¥ä½œåŸç†\n",
    "- åŒ»å­¦å›¾åƒçš„é¢„å¤„ç†\n",
    "- ç‰¹å¾å¯è§†åŒ–å’Œåˆ†æ\n",
    "- ResNetæ¶æ„è¯¦è§£\n",
    "\n",
    "### ğŸ’¡ æ€è€ƒé¢˜\n",
    "\n",
    "1. ä¸ºä»€ä¹ˆåŒ»å­¦æ–‡æœ¬éœ€è¦ç‰¹æ®Šçš„æ¸…ç†è§„åˆ™ï¼Ÿ\n",
    "2. å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯é¢‘é˜ˆå€¼ï¼Ÿ\n",
    "3. OOVé—®é¢˜å¯¹æ¨¡å‹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
    "4. å¦‚ä½•æ”¹è¿›tokenizerä»¥æ›´å¥½åœ°å¤„ç†åŒ»å­¦æœ¯è¯­ï¼Ÿ\n",
    "\n",
    "### ğŸ“š æ‰©å±•é˜…è¯»\n",
    "\n",
    "- [Subword tokenizationæ–¹æ³•](https://huggingface.co/docs/tokenizers/)\n",
    "- [åŒ»å­¦NLPçš„ç‰¹æ®ŠæŒ‘æˆ˜](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568148/)\n",
    "- [R2GenåŸè®ºæ–‡](https://arxiv.org/pdf/2010.16056.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **æ­å–œæ‚¨å®Œæˆäº†R2Genå­¦ä¹ ç³»åˆ—çš„ç¬¬ä¸€éƒ¨åˆ†ï¼**\n",
    "\n",
    "æ‚¨ç°åœ¨å·²ç»æŒæ¡äº†æ–‡æœ¬é¢„å¤„ç†å’Œtokenizationçš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºåç»­å­¦ä¹ æ‰“ä¸‹äº†åšå®çš„åŸºç¡€ã€‚ç»§ç»­ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œæˆ‘ä»¬åœ¨ä¸‹ä¸€ä¸ªnotebookä¸­è§ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
