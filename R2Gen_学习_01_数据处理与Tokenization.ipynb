{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2Gen学习系列 - 第1部分：数据处理与Tokenization\n",
    "\n",
    "## 🎯 学习目标\n",
    "在这个notebook中，我们将学习：\n",
    "1. R2Gen项目的背景和核心思想\n",
    "2. 医学报告生成的挑战\n",
    "3. 文本tokenization的原理和实现\n",
    "4. 词汇表构建过程\n",
    "5. 数据预处理的重要性\n",
    "\n",
    "## 📚 项目背景\n",
    "\n",
    "### 什么是R2Gen？\n",
    "R2Gen (Radiology Report Generation) 是一个基于深度学习的医学报告自动生成系统。它的核心创新是使用了**Memory-driven Transformer**架构，能够：\n",
    "\n",
    "- 📸 **分析医学影像**：处理X光片、CT扫描等医学图像\n",
    "- 🧠 **利用医学知识**：通过关系记忆模块存储和利用先验医学知识\n",
    "- 📝 **生成报告**：自动生成结构化的放射学报告\n",
    "\n",
    "### 为什么这很重要？\n",
    "1. **提高效率**：减少放射科医生的工作负担\n",
    "2. **标准化**：确保报告格式和术语的一致性\n",
    "3. **辅助诊断**：为医生提供初步的分析结果\n",
    "\n",
    "### 技术挑战\n",
    "- 医学术语的专业性和准确性要求\n",
    "- 图像特征与文本描述的对应关系\n",
    "- 长序列文本生成的连贯性\n",
    "- 医学知识的有效利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗺️ 学习路线图\n",
    "\n",
    "我们将通过以下步骤逐步学习R2Gen：\n",
    "\n",
    "```\n",
    "📖 第1部分：数据处理与Tokenization (当前)\n",
    "   ├── 文本预处理\n",
    "   ├── Tokenization原理\n",
    "   └── 词汇表构建\n",
    "\n",
    "🖼️ 第2部分：视觉特征提取\n",
    "   ├── CNN特征提取器\n",
    "   ├── 图像预处理\n",
    "   └── 特征可视化\n",
    "\n",
    "🔧 第3部分：Transformer基础组件\n",
    "   ├── 注意力机制\n",
    "   ├── 编码器-解码器\n",
    "   └── 位置编码\n",
    "\n",
    "🧠 第4部分：关系记忆模块\n",
    "   ├── 记忆机制原理\n",
    "   ├── 条件层归一化\n",
    "   └── 知识存储与检索\n",
    "\n",
    "🚀 第5部分：完整模型训练\n",
    "   ├── 损失函数\n",
    "   ├── 训练循环\n",
    "   └── 模型评估\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 数据集介绍\n",
    "\n",
    "R2Gen支持两个主要的医学数据集：\n",
    "\n",
    "### 1. IU X-Ray数据集\n",
    "- **图像类型**：胸部X光片\n",
    "- **特点**：每个病例包含两张图像（正面和侧面）\n",
    "- **报告**：结构化的放射学报告\n",
    "\n",
    "### 2. MIMIC-CXR数据集\n",
    "- **图像类型**：胸部X光片\n",
    "- **特点**：大规模数据集，单张图像\n",
    "- **报告**：详细的临床报告\n",
    "\n",
    "### 数据格式示例\n",
    "```json\n",
    "{\n",
    "  \"id\": \"patient_001\",\n",
    "  \"image_path\": [\"frontal.jpg\", \"lateral.jpg\"],\n",
    "  \"report\": \"The lungs are clear. No acute cardiopulmonary abnormality.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# 导入高分辨率配置\n",
    "try:\n",
    "    from matplotlib_config import setup_high_quality_plots, save_high_quality_figure, create_publication_figure, get_figure_size\n",
    "    setup_high_quality_plots()\n",
    "    print(\"✅ 已启用科研级别高分辨率绘图配置\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ matplotlib_config.py未找到，使用默认高分辨率配置\")\n",
    "    # 手动设置高分辨率\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    WORDCLOUD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WORDCLOUD_AVAILABLE = False\n",
    "    print(\"⚠️ WordCloud库未安装，词云功能将被跳过\")\n",
    "\n",
    "# 设置中文字体和图表样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"✅ 库导入完成！\")\n",
    "print(\"📚 准备开始学习R2Gen的数据处理模块...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔤 文本Tokenization详解\n",
    "\n",
    "### 什么是Tokenization？\n",
    "Tokenization是将原始文本转换为模型可以理解的数字序列的过程。在医学报告生成中，这个步骤尤为重要，因为：\n",
    "\n",
    "1. **医学术语处理**：正确处理专业医学词汇\n",
    "2. **标准化**：统一不同格式的报告\n",
    "3. **数值化**：将文本转换为神经网络可处理的数字\n",
    "\n",
    "### R2Gen的Tokenization流程\n",
    "```\n",
    "原始报告 → 文本清理 → 分词 → 词汇表映射 → 数字序列\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们从R2Gen源码中提取并简化Tokenizer类\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"简化版的R2Gen Tokenizer，用于学习目的\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=3):\n",
    "        \"\"\"\n",
    "        初始化tokenizer\n",
    "        \n",
    "        Args:\n",
    "            threshold: 词汇出现的最小频次，低于此频次的词会被标记为<unk>\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.token2idx = {}  # 词汇到索引的映射\n",
    "        self.idx2token = {}  # 索引到词汇的映射\n",
    "        \n",
    "    def clean_report_iu_xray(self, report):\n",
    "        \"\"\"\n",
    "        清理IU X-Ray数据集的报告文本\n",
    "        这个函数展示了医学文本预处理的复杂性\n",
    "        \"\"\"\n",
    "        # 第一步：处理句子分隔符和编号\n",
    "        report_cleaner = lambda t: t.replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '') \\\n",
    "            .replace('. 2. ', '. ').replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ') \\\n",
    "            .replace(' 2. ', '. ').replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "            .strip().lower().split('. ')\n",
    "        \n",
    "        # 第二步：清理每个句子中的标点符号\n",
    "        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\\\[\\\\]{}]', '', \n",
    "                                       t.replace('\"', '').replace('/', '').replace('\\\\\\\\', '').replace(\"'\", '').strip().lower())\n",
    "        \n",
    "        # 应用清理函数\n",
    "        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != '']\n",
    "        report = ' . '.join(tokens) + ' .'\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def create_vocabulary(self, reports):\n",
    "        \"\"\"\n",
    "        从报告列表中创建词汇表\n",
    "        \n",
    "        Args:\n",
    "            reports: 报告文本列表\n",
    "        \"\"\"\n",
    "        print(\"🔍 开始分析报告，构建词汇表...\")\n",
    "        \n",
    "        # 收集所有词汇\n",
    "        total_tokens = []\n",
    "        for report in reports:\n",
    "            cleaned_report = self.clean_report_iu_xray(report)\n",
    "            tokens = cleaned_report.split()\n",
    "            total_tokens.extend(tokens)\n",
    "        \n",
    "        # 统计词频\n",
    "        counter = Counter(total_tokens)\n",
    "        print(f\"📊 总共发现 {len(counter)} 个不同的词汇\")\n",
    "        print(f\"📊 总词汇数量: {len(total_tokens)}\")\n",
    "        \n",
    "        # 过滤低频词汇\n",
    "        vocab = [k for k, v in counter.items() if v >= self.threshold] + ['<unk>']\n",
    "        vocab.sort()\n",
    "        \n",
    "        print(f\"📊 过滤后词汇表大小: {len(vocab)} (阈值: {self.threshold})\")\n",
    "        \n",
    "        # 创建映射字典\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        \n",
    "        for idx, token in enumerate(vocab):\n",
    "            self.token2idx[token] = idx + 1  # 从1开始，0留给padding\n",
    "            self.idx2token[idx + 1] = token\n",
    "        \n",
    "        return counter\n",
    "    \n",
    "    def tokenize(self, report):\n",
    "        \"\"\"\n",
    "        将报告转换为token ID序列\n",
    "        \n",
    "        Args:\n",
    "            report: 原始报告文本\n",
    "            \n",
    "        Returns:\n",
    "            token ID列表\n",
    "        \"\"\"\n",
    "        cleaned_report = self.clean_report_iu_xray(report)\n",
    "        tokens = cleaned_report.split()\n",
    "        \n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token2idx:\n",
    "                ids.append(self.token2idx[token])\n",
    "            else:\n",
    "                ids.append(self.token2idx['<unk>'])  # 未知词汇\n",
    "        \n",
    "        # 添加开始和结束标记\n",
    "        ids = [0] + ids + [0]  # 0作为特殊标记\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        将token ID序列转换回文本\n",
    "        \n",
    "        Args:\n",
    "            ids: token ID列表\n",
    "            \n",
    "        Returns:\n",
    "            解码后的文本\n",
    "        \"\"\"\n",
    "        txt = ''\n",
    "        for i, idx in enumerate(ids):\n",
    "            if idx > 0:  # 跳过特殊标记\n",
    "                if i >= 1:\n",
    "                    txt += ' '\n",
    "                txt += self.idx2token[idx]\n",
    "            else:\n",
    "                break\n",
    "        return txt\n",
    "\n",
    "print(\"✅ SimpleTokenizer类定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 实践：使用Tokenizer处理医学报告\n",
    "\n",
    "现在让我们用一些示例医学报告来测试我们的tokenizer！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一些示例医学报告\n",
    "sample_reports = [\n",
    "    \"The lungs are clear. No acute cardiopulmonary abnormality. Heart size is normal.\",\n",
    "    \"Bilateral lower lobe pneumonia. Cardiomegaly is present. Pleural effusion noted.\",\n",
    "    \"No acute findings. Lungs are clear bilaterally. Normal heart size and contour.\",\n",
    "    \"Pneumothorax on the right side. Lung collapse observed. Immediate attention required.\",\n",
    "    \"Chronic obstructive pulmonary disease. Hyperinflation of lungs. No acute changes.\",\n",
    "    \"Normal chest X-ray. No abnormalities detected. Heart and lungs appear normal.\",\n",
    "    \"Pulmonary edema present. Enlarged heart shadow. Bilateral infiltrates seen.\",\n",
    "    \"Rib fractures on left side. No pneumothorax. Lungs are otherwise clear.\"\n",
    "]\n",
    "\n",
    "print(\"📋 示例医学报告准备完成！\")\n",
    "print(f\"📊 总共 {len(sample_reports)} 份报告\")\n",
    "print(\"\\n📝 报告示例:\")\n",
    "for i, report in enumerate(sample_reports[:3], 1):\n",
    "    print(f\"{i}. {report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建tokenizer实例并构建词汇表\n",
    "tokenizer = SimpleTokenizer(threshold=2)  # 设置较低的阈值以便观察更多词汇\n",
    "\n",
    "# 构建词汇表\n",
    "word_counter = tokenizer.create_vocabulary(sample_reports)\n",
    "\n",
    "print(f\"\\n📚 词汇表构建完成！\")\n",
    "print(f\"📊 词汇表大小: {len(tokenizer.token2idx)}\")\n",
    "print(f\"📊 最高频词汇: {word_counter.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化词频分布\n",
    "# 使用科研级别的图片尺寸\n",
    "try:\n",
    "    fig = create_publication_figure(figsize=get_figure_size('presentation'))\n",
    "except NameError:\n",
    "    fig = plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "# 子图1：词频分布\n",
    "plt.subplot(2, 2, 1)\n",
    "top_words = word_counter.most_common(15)\n",
    "words, counts = zip(*top_words)\n",
    "plt.barh(range(len(words)), counts)\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('出现频次')\n",
    "plt.title('Top 15 高频词汇')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# 子图2：词汇长度分布\n",
    "plt.subplot(2, 2, 2)\n",
    "word_lengths = [len(word) for word in word_counter.keys()]\n",
    "plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('词汇长度')\n",
    "plt.ylabel('词汇数量')\n",
    "plt.title('词汇长度分布')\n",
    "\n",
    "# 子图3：词频分布直方图\n",
    "plt.subplot(2, 2, 3)\n",
    "freq_counts = list(word_counter.values())\n",
    "plt.hist(freq_counts, bins=range(1, max(freq_counts)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('出现频次')\n",
    "plt.ylabel('词汇数量')\n",
    "plt.title('词频分布直方图')\n",
    "plt.yscale('log')\n",
    "\n",
    "# 子图4：词云（如果可用）\n",
    "plt.subplot(2, 2, 4)\n",
    "if WORDCLOUD_AVAILABLE:\n",
    "    # 提高词云分辨率\n",
    "    wordcloud = WordCloud(width=800, height=600, background_color='white', max_words=100).generate_from_frequencies(word_counter)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('医学词汇词云')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'WordCloud\\\\n库未安装', ha='center', va='center', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.title('词云功能不可用')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存高分辨率图片\n",
    "try:\n",
    "    save_high_quality_figure('vocabulary_analysis_high_res.png')\n",
    "except NameError:\n",
    "    plt.savefig('vocabulary_analysis_high_res.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(\"💾 高分辨率图片已保存: vocabulary_analysis_high_res.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 词汇分析可视化完成！\")\n",
    "print(\"🎯 图片已保存为科研级别高分辨率格式 (300 DPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Tokenization过程演示\n",
    "\n",
    "让我们详细观察一个报告是如何被tokenize的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一个示例报告进行详细分析\n",
    "example_report = \"Bilateral lower lobe pneumonia. Cardiomegaly is present. Pleural effusion noted.\"\n",
    "\n",
    "print(\"🔍 Tokenization过程演示\")\n",
    "print(\"=\"*50)\n",
    "print(f\"📝 原始报告: {example_report}\")\n",
    "print()\n",
    "\n",
    "# 步骤1：文本清理\n",
    "cleaned = tokenizer.clean_report_iu_xray(example_report)\n",
    "print(f\"🧹 清理后: {cleaned}\")\n",
    "print()\n",
    "\n",
    "# 步骤2：分词\n",
    "tokens = cleaned.split()\n",
    "print(f\"🔤 分词结果: {tokens}\")\n",
    "print(f\"📊 词汇数量: {len(tokens)}\")\n",
    "print()\n",
    "\n",
    "# 步骤3：转换为ID\n",
    "token_ids = tokenizer.tokenize(example_report)\n",
    "print(f\"🔢 Token IDs: {token_ids}\")\n",
    "print()\n",
    "\n",
    "# 步骤4：解码验证\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"🔄 解码结果: {decoded}\")\n",
    "print()\n",
    "\n",
    "# 详细的词汇映射\n",
    "print(\"📋 详细的词汇映射:\")\n",
    "print(\"-\" * 30)\n",
    "for i, token in enumerate(tokens):\n",
    "    token_id = tokenizer.token2idx.get(token, tokenizer.token2idx['<unk>'])\n",
    "    print(f\"{i+1:2d}. '{token}' → {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 本节总结\n",
    "\n",
    "### 🎯 我们学到了什么？\n",
    "\n",
    "1. **R2Gen项目背景**\n",
    "   - 医学报告自动生成的重要性\n",
    "   - Memory-driven Transformer的核心思想\n",
    "   - 技术挑战和解决方案\n",
    "\n",
    "2. **文本预处理的重要性**\n",
    "   - 医学文本的特殊性\n",
    "   - 标准化处理的必要性\n",
    "   - 清理规则的设计原则\n",
    "\n",
    "3. **Tokenization核心概念**\n",
    "   - 词汇表构建策略\n",
    "   - 频次阈值的作用\n",
    "   - 未知词汇的处理\n",
    "\n",
    "4. **实践技能**\n",
    "   - 实现简化版tokenizer\n",
    "   - 词频分析和可视化\n",
    "   - 性能评估方法\n",
    "\n",
    "### 🔑 关键要点\n",
    "\n",
    "- **数据质量决定模型性能**：好的预处理是成功的一半\n",
    "- **领域特异性**：医学文本需要专门的处理策略\n",
    "- **平衡性考虑**：词汇表大小与覆盖率的权衡\n",
    "- **可解释性**：每个处理步骤都应该是可理解和可验证的\n",
    "\n",
    "### 🚀 下一步学习计划\n",
    "\n",
    "在下一个notebook中，我们将学习：\n",
    "\n",
    "📸 **视觉特征提取**\n",
    "- CNN特征提取器的工作原理\n",
    "- 医学图像的预处理\n",
    "- 特征可视化和分析\n",
    "- ResNet架构详解\n",
    "\n",
    "### 💡 思考题\n",
    "\n",
    "1. 为什么医学文本需要特殊的清理规则？\n",
    "2. 如何选择合适的词频阈值？\n",
    "3. OOV问题对模型性能有什么影响？\n",
    "4. 如何改进tokenizer以更好地处理医学术语？\n",
    "\n",
    "### 📚 扩展阅读\n",
    "\n",
    "- [Subword tokenization方法](https://huggingface.co/docs/tokenizers/)\n",
    "- [医学NLP的特殊挑战](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568148/)\n",
    "- [R2Gen原论文](https://arxiv.org/pdf/2010.16056.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "🎉 **恭喜您完成了R2Gen学习系列的第一部分！**\n",
    "\n",
    "您现在已经掌握了文本预处理和tokenization的核心概念，为后续学习打下了坚实的基础。继续保持学习的热情，我们在下一个notebook中见！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
