#!/usr/bin/env python3
"""
MIMIC-CXRè®­ç»ƒè„šæœ¬ - åŸºäºä¿®å¤åçš„R2Genä»£ç 
ä½¿ç”¨FP32ç²¾åº¦ç¡®ä¿æœ€ä½³è´¨é‡ï¼Œä¸¥æ ¼éµå¾ªè®ºæ–‡é…ç½®
"""

import os
import sys
import torch
import argparse
import numpy as np
import wandb
from datetime import datetime
import json
import time

# æ·»åŠ R2Genè·¯å¾„
sys.path.append('R2Gen-main')

from modules.tokenizers import Tokenizer
from modules.dataloaders import R2DataLoader
from modules.metrics import compute_scores
from modules.optimizers import build_optimizer, build_lr_scheduler
from modules.trainer import Trainer
from modules.loss import compute_loss
from models.r2gen import R2GenModel

def setup_wandb(args):
    """è®¾ç½®WandBç›‘æ§"""
    wandb.init(
        project="R2Gen-MIMIC-CXR-Fixed",
        name=f"mimic_cxr_fp32_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config={
            "dataset": "MIMIC-CXR",
            "precision": "FP32",
            "batch_size": args.batch_size,
            "epochs": args.epochs,
            "lr_ve": args.lr_ve,
            "lr_ed": args.lr_ed,
            "step_size": args.step_size,
            "gamma": args.gamma,
            "d_model": args.d_model,
            "num_heads": args.num_heads,
            "num_layers": args.num_layers,
            "max_seq_length": args.max_seq_length,
            "beam_size": args.beam_size,
            "seed": args.seed
        },
        tags=["MIMIC-CXR", "FP32", "Fixed-BatchNorm", "Paper-Config"]
    )

def log_gpu_memory():
    """è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        memory_free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3
        
        wandb.log({
            "gpu_memory_allocated_gb": memory_allocated,
            "gpu_memory_reserved_gb": memory_reserved,
            "gpu_memory_free_gb": memory_free,
            "gpu_memory_utilization": memory_allocated / (torch.cuda.get_device_properties(0).total_memory / 1024**3) * 100
        })
        
        return memory_allocated, memory_reserved, memory_free
    return 0, 0, 0

def parse_args():
    parser = argparse.ArgumentParser(description='MIMIC-CXRè®­ç»ƒ - ä¿®å¤ç‰ˆR2Gen')

    # æ•°æ®è®¾ç½®
    parser.add_argument('--image_dir', type=str, default='R2Gen-main/data/mimic_cxr/images/',
                        help='å›¾åƒç›®å½•è·¯å¾„')
    parser.add_argument('--ann_path', type=str, default='R2Gen-main/data/mimic_cxr/annotation.json',
                        help='æ ‡æ³¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset_name', type=str, default='mimic_cxr',
                        help='æ•°æ®é›†åç§°')

    # è§†è§‰æå–å™¨è®¾ç½®
    parser.add_argument('--visual_extractor', type=str, default='resnet101',
                        help='è§†è§‰ç‰¹å¾æå–å™¨')
    parser.add_argument('--visual_extractor_pretrained', type=bool, default=True,
                        help='æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æå–å™¨')

    # æ¨¡å‹è®¾ç½® - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡é…ç½®
    parser.add_argument('--d_model', type=int, default=512,
                        help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--num_heads', type=int, default=8,
                        help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--num_layers', type=int, default=3,
                        help='Transformerå±‚æ•°')
    parser.add_argument('--d_ff', type=int, default=512,
                        help='å‰é¦ˆç½‘ç»œç»´åº¦')
    parser.add_argument('--d_vf', type=int, default=2048,
                        help='è§†è§‰ç‰¹å¾ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.1,
                        help='Transformer dropoutç‡')
    parser.add_argument('--logit_layers', type=int, default=1,
                        help='logitå±‚æ•°')

    # RelationalMemoryè®¾ç½®
    parser.add_argument('--rm_num_slots', type=int, default=3,
                        help='RelationalMemoryæ§½ä½æ•°')
    parser.add_argument('--rm_num_heads', type=int, default=8,
                        help='RelationalMemoryæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--rm_d_model', type=int, default=512,
                        help='RelationalMemoryç»´åº¦')

    # ç‰¹æ®Štokenè®¾ç½®
    parser.add_argument('--bos_idx', type=int, default=0,
                        help='å¼€å§‹tokenç´¢å¼•')
    parser.add_argument('--eos_idx', type=int, default=0,
                        help='ç»“æŸtokenç´¢å¼•')
    parser.add_argument('--pad_idx', type=int, default=0,
                        help='å¡«å……tokenç´¢å¼•')

    # é‡‡æ ·è®¾ç½®
    parser.add_argument('--sample_method', type=str, default='beam_search',
                        help='é‡‡æ ·æ–¹æ³•')
    parser.add_argument('--beam_size', type=int, default=3,
                        help='æŸæœç´¢å¤§å°')
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--sample_n', type=int, default=1,
                        help='æ¯å¼ å›¾åƒçš„é‡‡æ ·æ•°')
    parser.add_argument('--group_size', type=int, default=1,
                        help='ç»„å¤§å°')
    parser.add_argument('--output_logsoftmax', type=int, default=1,
                        help='æ˜¯å¦è¾“å‡ºlog softmax')
    parser.add_argument('--decoding_constraint', type=int, default=0,
                        help='æ˜¯å¦ä½¿ç”¨è§£ç çº¦æŸ')
    parser.add_argument('--block_trigrams', type=int, default=1,
                        help='æ˜¯å¦é˜»æ­¢ä¸‰å…ƒç»„é‡å¤')

    # è®­ç»ƒè®¾ç½® - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡é…ç½®
    parser.add_argument('--lr_ve', type=float, default=5e-5,
                        help='Visual Encoderå­¦ä¹ ç‡')
    parser.add_argument('--lr_ed', type=float, default=1e-4,
                        help='Encoder-Decoderå­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=5e-5,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--amsgrad', type=bool, default=True,
                        help='æ˜¯å¦ä½¿ç”¨AMSGrad')
    parser.add_argument('--optim', type=str, default='Adam',
                        help='ä¼˜åŒ–å™¨ç±»å‹')
    parser.add_argument('--lr_scheduler', type=str, default='StepLR',
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹')
    parser.add_argument('--step_size', type=int, default=1,
                        help='å­¦ä¹ ç‡è¡°å‡æ­¥é•¿')
    parser.add_argument('--gamma', type=float, default=0.8,
                        help='å­¦ä¹ ç‡è¡°å‡å› å­')
    parser.add_argument('--epochs', type=int, default=30,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')

    # åºåˆ—è®¾ç½®
    parser.add_argument('--max_seq_length', type=int, default=100,
                        help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--threshold', type=int, default=10,
                        help='è¯é¢‘é˜ˆå€¼')

    # è®­ç»ƒå™¨è®¾ç½®
    parser.add_argument('--n_gpu', type=int, default=1,
                        help='GPUæ•°é‡')
    parser.add_argument('--save_period', type=int, default=1,
                        help='ä¿å­˜å‘¨æœŸ')
    parser.add_argument('--monitor_mode', type=str, default='max',
                        help='ç›‘æ§æ¨¡å¼')
    parser.add_argument('--monitor_metric', type=str, default='BLEU_4',
                        help='ç›‘æ§æŒ‡æ ‡')
    parser.add_argument('--early_stop', type=int, default=50,
                        help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--record_dir', type=str, default='records/',
                        help='è®°å½•ç›®å½•')

    # å…¶ä»–è®¾ç½®
    parser.add_argument('--seed', type=int, default=456789,
                        help='éšæœºç§å­')
    parser.add_argument('--save_dir', type=str, default='results/mimic_cxr_fixed',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, default=None,
                        help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--use_bn', type=int, default=1,
                        help='æ˜¯å¦ä½¿ç”¨BatchNorm')
    parser.add_argument('--drop_prob_lm', type=float, default=0.5,
                        help='è¯­è¨€æ¨¡å‹dropoutæ¦‚ç‡')

    return parser.parse_args()

def validate_environment():
    """éªŒè¯è®­ç»ƒç¯å¢ƒ"""
    print("ğŸ” éªŒè¯è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"âœ… GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    args = parse_args()
    if not os.path.exists(args.ann_path):
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {args.ann_path}")
        return False
    
    if not os.path.exists(args.image_dir):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {args.image_dir}")
        return False
    
    print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # æ£€æŸ¥ä¿®å¤çš„BatchNorm
    try:
        from modules.att_model import FixedBatchNorm1dWrapper
        print("âœ… BatchNormä¿®å¤å·²åº”ç”¨")
    except ImportError:
        print("âŒ BatchNormä¿®å¤æœªæ‰¾åˆ°")
        return False
    
    return True

def create_model_and_tokenizer(args):
    """åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = Tokenizer(args)
    
    # åˆ›å»ºæ¨¡å‹
    model = R2GenModel(args, tokenizer)
    
    # ç§»åŠ¨åˆ°GPU
    if torch.cuda.is_available():
        model = model.cuda()
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model, tokenizer

def create_data_loaders(args, tokenizer):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    train_dataloader = R2DataLoader(args, tokenizer, split='train', shuffle=True)
    val_dataloader = R2DataLoader(args, tokenizer, split='val', shuffle=False)
    test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_dataloader)} æ‰¹æ¬¡")
    print(f"   éªŒè¯é›†: {len(val_dataloader)} æ‰¹æ¬¡")
    print(f"   æµ‹è¯•é›†: {len(test_dataloader)} æ‰¹æ¬¡")
    
    return train_dataloader, val_dataloader, test_dataloader

def create_optimizer_and_scheduler(args, model):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    print("âš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨...")
    
    optimizer = build_optimizer(args, model)
    lr_scheduler = build_lr_scheduler(args, optimizer)
    
    print(f"âœ… ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    print(f"   Visual Encoder LR: {args.lr_ve}")
    print(f"   Encoder-Decoder LR: {args.lr_ed}")
    print(f"   è¡°å‡ç­–ç•¥: æ¯{args.step_size}ä¸ªepochä¹˜ä»¥{args.gamma}")
    
    return optimizer, lr_scheduler

def train_epoch(model, dataloader, optimizer, criterion, epoch, args):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    num_batches = len(dataloader)
    
    print(f"ğŸš€ å¼€å§‹ç¬¬{epoch}ä¸ªepochè®­ç»ƒ...")
    start_time = time.time()
    
    for batch_idx, (images_id, images, reports_ids, reports_masks) in enumerate(dataloader):
        if torch.cuda.is_available():
            images = images.cuda()
            reports_ids = reports_ids.cuda()
            reports_masks = reports_masks.cuda()
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(images, reports_ids, mode='train')
        loss = criterion(output, reports_ids, reports_masks)
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_value_(model.parameters(), 0.1)
        optimizer.step()
        
        total_loss += loss.item()
        
        # è®°å½•è®­ç»ƒè¿›åº¦
        if batch_idx % 10 == 0:
            current_lr_ve = optimizer.param_groups[0]['lr']
            current_lr_ed = optimizer.param_groups[1]['lr']
            
            wandb.log({
                "train_loss": loss.item(),
                "lr_ve": current_lr_ve,
                "lr_ed": current_lr_ed,
                "epoch": epoch,
                "batch": batch_idx
            })
            
            # è®°å½•GPUå†…å­˜
            mem_alloc, mem_reserved, mem_free = log_gpu_memory()
            
            print(f"Epoch {epoch}, Batch {batch_idx}/{num_batches}, "
                  f"Loss: {loss.item():.4f}, "
                  f"LR_VE: {current_lr_ve:.2e}, LR_ED: {current_lr_ed:.2e}, "
                  f"GPU: {mem_alloc:.1f}GB")
    
    avg_loss = total_loss / num_batches
    epoch_time = time.time() - start_time
    
    print(f"âœ… Epoch {epoch} è®­ç»ƒå®Œæˆ")
    print(f"   å¹³å‡Loss: {avg_loss:.4f}")
    print(f"   è®­ç»ƒæ—¶é—´: {epoch_time:.1f}ç§’")
    
    return avg_loss

def evaluate_model(model, dataloader, tokenizer, split_name):
    """è¯„ä¼°æ¨¡å‹"""
    print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹ ({split_name})...")
    
    model.eval()
    reports_generated = []
    reports_ground_truth = []
    
    with torch.no_grad():
        for images_id, images, reports_ids, reports_masks in dataloader:
            if torch.cuda.is_available():
                images = images.cuda()
            
            # ç”ŸæˆæŠ¥å‘Š
            output = model(images, mode='sample')
            reports = model.tokenizer.decode_batch(output.cpu().numpy())
            ground_truths = model.tokenizer.decode_batch(reports_ids.numpy())
            
            reports_generated.extend(reports)
            reports_ground_truth.extend(ground_truths)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    gts = {i: [gt] for i, gt in enumerate(reports_ground_truth)}
    res = {i: [gen] for i, gen in enumerate(reports_generated)}
    scores = compute_scores(gts, res)
    
    print(f"âœ… {split_name}è¯„ä¼°å®Œæˆ:")
    for metric, score in scores.items():
        print(f"   {metric}: {score:.4f}")
    
    return scores, reports_generated[:5]  # è¿”å›å‰5ä¸ªç”Ÿæˆæ ·æœ¬

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹MIMIC-CXRè®­ç»ƒå®éªŒ...")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        print("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥ï¼Œå®éªŒç»ˆæ­¢")
        return
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    # è®¾ç½®WandB
    setup_wandb(args)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = create_model_and_tokenizer(args)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader, val_dataloader, test_dataloader = create_data_loaders(args, tokenizer)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, lr_scheduler = create_optimizer_and_scheduler(args, model)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = compute_loss
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ - {args.epochs} epochs")
    print("="*60)
    
    best_val_score = 0
    best_epoch = 0
    
    for epoch in range(1, args.epochs + 1):
        print(f"\nğŸ“… Epoch {epoch}/{args.epochs}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, epoch, args)
        
        # éªŒè¯
        val_scores, val_samples = evaluate_model(model, val_dataloader, tokenizer, 'validation')
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr_scheduler.step()
        
        # è®°å½•åˆ°WandB
        wandb.log({
            "epoch": epoch,
            "train_loss": train_loss,
            **{f"val_{k}": v for k, v in val_scores.items()}
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_score = val_scores.get('BLEU_4', 0)
        if current_score > best_val_score:
            best_val_score = current_score
            best_epoch = epoch
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(args.save_dir, 'model_best.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_scores': val_scores,
                'args': args
            }, model_path)
            
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! BLEU_4: {current_score:.4f}")
        
        print(f"å½“å‰æœ€ä½³: Epoch {best_epoch}, BLEU_4: {best_val_score:.4f}")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*60)
    
    # æœ€ç»ˆæµ‹è¯•
    print("ğŸ“Š æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_model_path = os.path.join(args.save_dir, 'model_best.pth')
    checkpoint = torch.load(best_model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # æµ‹è¯•è¯„ä¼°
    test_scores, test_samples = evaluate_model(model, test_dataloader, tokenizer, 'test')
    
    # è®°å½•æœ€ç»ˆç»“æœ
    wandb.log({
        **{f"final_test_{k}": v for k, v in test_scores.items()},
        "best_epoch": best_epoch,
        "best_val_bleu4": best_val_score
    })
    
    # ä¿å­˜ç»“æœ
    results = {
        'args': vars(args),
        'best_epoch': best_epoch,
        'best_val_scores': checkpoint['val_scores'],
        'final_test_scores': test_scores,
        'test_samples': test_samples,
        'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    results_path = os.path.join(args.save_dir, 'training_results.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ† æœ€ç»ˆç»“æœ:")
    print(f"æœ€ä½³éªŒè¯BLEU_4: {best_val_score:.4f} (Epoch {best_epoch})")
    print(f"æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    for metric, score in test_scores.items():
        print(f"  {metric}: {score:.4f}")
    
    wandb.finish()

if __name__ == "__main__":
    main()
